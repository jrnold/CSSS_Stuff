A Covariance Regression Model
Peter D. Hoﬀ1 and Xiaoyue Niu2
Working Paper no. 96
Center for Statistics and the Social Sciences
University of Washington
Seattle, WA 98195-4322.

December 28, 2009

Abstract
Classical regression analysis relates the expectation of a response variable to a linear combination of
explanatory variables. In this article, we propose a covariance regression model that parameterizes
the covariance matrix of a multivariate response vector as a parsimonious quadratic function of
explanatory variables. The approach can be seen as analogous to the mean regression model,
and has a representation as a type of random eﬀects model. Parameter estimation for covariance
regression is straightforward using either an EM algorithm or a Gibbs sampling scheme. The
proposed methodology provides a simple but flexible representation of heteroscedasticity across the
levels of an explanatory variable, and can give better-calibrated prediction regions when compared
to a homoscedastic model.

Departments of Statistics1,2 and Biostatistics1 , University of Washington, Seattle, WA 98195-4322. Web: www.
stat.washington.edu/~hoff. This work was partially supported by NSF grant SES-0631531.

Some key words: heteroscedasticity, Markov chain Monte Carlo, multivariate, positive definite cone,
random eﬀects.

1

Introduction

Estimation of a conditional mean function µx = E[y|x] is a well studied data-analysis task for which
there are a large number of statistical models and procedures. Less studied is the problem of estimating a covariance function Σx = Cov[y|x] across a range of values of an explanatory x-variable.
In the univariate case, several procedures assume that the variance can be expressed as a function
of the mean, i.e. σx2 = g(µx ) for some known function g (see, for example, Carroll et al. [1982]).
In many such cases the data can be represented by a generalized linear model with an appropriate
variance function, or perhaps the data can be transformed to a scale in which the variance is constant as a function of the mean [Box and Cox, 1964]. Other approaches separately parameterize
the mean and variance, giving either a linear model for the standard deviation [Rutemiller and
Bowers, 1968] or by forcing the variance to be non-negative via a link function [Smyth, 1989]. In
situations where the explanatory variable x is continuous and the variance function assumed to be
smooth, Carroll [1982] and M¨
uller and Stadtm¨
uller [1987] propose and study kernel estimates of
the variance function.
Less developed are methods for multivariate heteroscedasticity. One exception is in the context of multivariate time series, for which a variety of multivariate “autoregressive conditionally
heteroscedastic” (ARCH) models have been developed [Engle and Kroner, 1995, Fong et al., 2006].
However, the applicability of such models are limited to situations where the heteroscedasticity is
temporal in nature.
In this article we develop a simple model for a covariance function {Σx : x ∈ X } for which the

domain of the explanatory x-variable is the same as in mean regression, that is, the explanatory
vector can contain continuous, discrete and categorical variables. Our model is based on an analogy
with linear regression. As a function of x, the covariance regression function Σx is a curve within
the cone of positive definite matrices. A geometric interpretation of this model is developed in
Section 2, along with a representation as a random eﬀects model. Section 3 discusses methods of
parameter estimation, including an EM algorithm for obtaining maximum likelihood estimates, as
well as a Gibbs sampler for Bayesian inference. Section 4 illustrates the model with a simple data
analysis involving a bivariate response vector and a univariate continuous explanatory variable.
Section 5 summarizes the article and suggests directions for further research.

1

2

A covariance regression model

2.1

Model definition and geometry

Let y ∈ Rp be a random multivariate response vector and x ∈ Rq be a vector of explanatory

variables. Our goal is to provide a parsimonious model and estimation method for Cov[y|x] = Σx ,
the conditional covariance matrix of y given x. We begin by analogy with linear regression. The
simple linear regression model expresses the conditional mean µx = E[y|x] as a + Bx, an aﬃne
function of x. This model restricts the p-dimensional vector µx to a q-dimensional subspace of
Rp . The set of p × p covariance matrices is the cone of positive semidefinite matrices. This cone is

convex and thus closed under addition. The simplest version of our proposed covariance regression
model expresses Σx as
Σx = A + BxxT BT

(1)

where A is a p×p positive-definite matrix and B is a p×q matrix. The resulting covariance function
is positive definite for all x, and expresses the covariance as equal to a “baseline” covariance matrix
A plus a rank-1, p × p positive definite matrix that depends on x. The model given by Equation 1

is in some sense a natural generalization of mean regression to a model for covariance matrices. A
vector mean function lies in a vector (linear) space, and is expressed as a linear map from Rq to Rp .
The covariance matrix function lies in the cone of positive definite matrices, where the natural group
action is matrix multiplication on the left and right. The covariance regression model expresses the
covariance function via such a map from the q × q cone to the p × p cone.

Letting {b1 , . . . , bp } be the rows of B, the covariance regression model gives
Var[yj |x] = aj,j + bTj xxT bj

Cov[yj , yk |x] = aj,k + bTj xxT bk .

(2)
(3)

The parameterization of the variance suggests that the model requires the variance of each element
of y to be increasing in the absolute value of the elements of x, as the minimum variance is
obtained when x = 0. This constraint can be alleviated by including an intercept term so that the
first element of the explanatory vector is 1. For example, in the case of a single scalar explanatory
variable x we write bj = (b1,j , b2,j )T , giving
Var[yj |x] = aj,j + (b2,j + b2,j x)2

Cov[yj , yk |x] = aj,k + (b1,j + b2,j x)(b1,k + b2,k x).
For any given finite interval (c, d) ⊂ R there exist parameter values (b1,j , b2,j ) so that the variance
of yj is either increasing or decreasing in x for x ∈ (c, d).

We now consider the geometry of the covariance regression model. For each x, the model ex-

presses Σx as equal to a point A inside the positive-definite cone plus a rank-1 positive-semidefinite
2

matrix BxxT BT . The latter matrix is a point on the boundary of the cone, so the range of Σx as
a function of x can be seen as a submanifold of the boundary of the cone, but “pushed into” the
cone by an amount A. Figure 1 represents this graphically for the simplest of cases, in which p = 2
and there is just a single scalar explanatory variable x. In this case, each covariance matrix can be
expressed as a three-dimensional vector (σ12 , σ22 , σ1,2 } such that
σ12 ≥ 0 , σ22 ≥ 0 , |σ1,2 | ≤ σ1 σ2 .
The set of such points constitutes the positive semidefinite cone, whose boundary is shown by
the outer surfaces in the two plots in Figure 1. The range of BxxT BT over all x and matrices

Figure 1: The positive-definite cone and a translation, from two perspectives. The outer surface is
the boundary of the the positive definite cone, and the inner cone is equal to the boundary plus a
positive definite matrix A. Black curves on the inner cone represent covariance regression curves
A + BxxT BT for diﬀerent values of B.
B includes the set of all rank-1 positive definite matrices, which is simply the boundary of the
cone. Thus the possible range of A + BxxT BT for a given A is simply the boundary of the cone,
translated by an amount A. Such a translated cone is shown from two perspectives in Figure 1.
For a given A and B, the covariance regression model expresses Σx as a curve on this translated
boundary. A few such curves for six diﬀerent values of B are shown in black in Figure 1.

3

2.2

Random eﬀects representation

The covariance regression model also has an interpretation as a type of random eﬀects model.
Consider a model for observed data y1 , . . . , yn of the following form:
yi = µxi + γi × Bxi + �i

(4)

E[�i ] = 0 , Cov[�i ] = A

E[γi ] = 0 , Var[γi ] = 1 , E[γi × �i ] = 0.
The resulting covariance matrix for yi given xi is then
E[(yi − µxi )(yi − µxi )T ] = E[γi2 Bxi xTi BT + γi (Bxi �Ti + �i xTi BT ) + �i �Ti ]
= Bxi xTi BT + A
= Σxi .
The model given by Equation 4 can be thought of as a factor analysis model in which the latent
factor for unit i is restricted to be a multiple of the unit’s explanatory vector xi . To see how this
impacts the variance, let {b1 , . . . , bp } be the rows of B. The model in (4) can then be expressed as




yi,1 − µxi ,1
bT1 xi



..

 = γi ×  ...
.



yi,p − µxi ,p
bTp xi






�i,1
  . 
 +  ..  .
 

�i,p

(5)

We can interpret γi as describing additional unit-level variability beyond that represented by �i .
The vectors {b1 , . . . , bp } describe how this additional variability is manifested across the p diﬀerent
response variables.

Via the above random eﬀects representation, the covariance regression model can be seen as
similar in spirit to a random eﬀects model for longitudinal data discussed in Scott and Handcock
[2001]. In that article, the covariance among a set of repeated measurements yi from a single
individual i were modeled as yi = µi + γi Xi β + �i , where Xi is an observed design matrix for
the repeated measurements and γi is a mean-zero unit variance random eﬀect. In the longitudinal
data application in that article, Xi was constructed from a set of basis functions evaluated at the
observed time points, and β represented unknown weights. This model induces a covariance matrix
of Xi ββ T XTi + Cov[�i ] among the observations common to an individual. For the problem we are
considering in this article, where the explanatory variables are shared among all p observations of
a given unit (i.e. the rows of Xi are identical and equal to xi ), the covariance matrix induced by
Scott and Handcock’s model reduces to (xTi β)2 11T + Cov[�i ], which is much more restrictive than
the model given by (4).

4

2.3

Higher rank models

The model given by Equation 1 restricts the diﬀerence between Σx and the baseline matrix A to be
a rank-one matrix. This restriction can be lifted by extending the model to allow for higher-rank
deviations. Consider the following extension of the random eﬀects representation given by Equation
4:
y = µx + γ × Bx + ψ × Cx + �

(6)

where γ and ψ are mean-zero variance-one random variables, uncorrelated with each other and
with �. Under this model, the covariance of y is given by
Σx = A + BxxT BT + CxxT CT .
This model allows the deviation of Σx from the baseline A to be of rank 2. Additionally, we can
interpret the second random eﬀect ψ as allowing an additional, independent source of heteroscedasticity for the set of the p response variables. For the rank-2 model, Equation 5





 
yi,1 − µxi ,1
bT1 xi
cT1 xi
�i,1


 . 
 .   .
.

 = γi ×  ..  + ψi ×  ..  +  ..
..





 
yi,p − µxi ,p
bTp xi
cTp xi
�i,p

becomes


.


Whereas the rank-1 model essentially requires that extreme residuals for one element of y co-occur
with extreme residuals of the other elements, the rank-2 model provides more flexibility, allowing for
heteroscedasticity across multiple elements of y without requiring extreme residuals for all or none
of the elements. Further flexibility can be gained by adding additional random eﬀects, allowing the
diﬀerence between Σx and the baseline A to be of any desired rank.

2.4

Identifiability

We first consider identifiability for the rank-1 model and a single scalar explanatory variable x.
Including an intercept term so that the explanatory vector is (1, x)T , the model in (1) becomes
Σx (A, B) = A + b1 bT1 + (b1 bT2 + b2 bT1 )x + b2 bT2 x2 .
˜ B)
˜ are such that Σx (A, B) = Σx (A,
˜ B)
˜ for all x ∈ R. Setting x = 0
Now suppose that (A,
T
˜ 1b
˜ . Considering x = ±1 implies that b2 bT = b
˜ 2b
˜ T and thus
˜ +b
indicates that A + b1 bT1 = A
1
2
2
˜ 2 = ±b2 . If b2 �= 0, we have b1 bT + b2 bT = b˜1 b˜2 T + b˜2 b˜1 T , which implies that B
˜ = ±B
that b
2

1

˜ = A. Thus these parameters are identifiable, at least given an adequate range of x-values.
and A
�

For the rank-r model with r > 1, consider a random eﬀects representation given by yi − µxi =
(1)

(r)

γi,k × B(k) xi + �i . Let B1 = (b1 , . . . , b1 ) be the p × r matrix defined by the first columns of
5

B(1) , . . . , B(r) , and define {Bj : k = 1, . . . , q} similarly. The model can then be expressed as
yi − µxi =

q
�

xk Bk γ i + �i .

k=1

Now suppose that γ i is allowed to have a covariance matrix Ψ not necessarily equal to the identity. The above representation shows that the model given by {B1 , . . . , Bk , Ψ} is equivalent to

the one given by {B1 Ψ1/2 , . . . , Bk Ψ1/2 , I}, and so without loss of generality it can be assumed
that Ψ = I, i.e. the random eﬀects are independent with unit variance. In this case, note that
Cov[γ i ] = Cov[Hγ i ] where H is any r × r orthonormal matrix. This implies that the covariance

function Σx given by {B1 , . . . , Bk , I} is equal to the one given by {B1 H, . . . , Bk H, I} for any orthonormal H, and so the parameters in the higher rank model are not completely identifiable. One
(1)

(r)

possible identifiability constraint is to restrict B1 = (b1 , . . . , b1 ), the matrix of first columns of
B(1) , . . . , B(r) , to have orthogonal columns.

3
3.1

Parameter estimation
Likelihood-based inference

In this section we consider parameter estimation based on the n × p data matrix Y = (y1 , . . . , yn )T
observed under conditions X = (x1 , . . . , xn )T . We assume normal models for all error terms:
γ1 , . . . , γn ∼ independent normal(0, 1)

(7)

�1 , . . . , �n ∼ independent multivariate normal(0, A)
yi = µxi + γi × Bxi + �i .

For now, assume {µx , x ∈ X } are known and let E = (e1 , . . . , en )T be the n × p matrix of residuals.
The log likelihood of the parameters based on E and X is
1�
1�
l(A, B : E, X) = c −
log |A + Bxi xTi B| −
tr[(A + Bxi xTi BT )−1 ei eTi ].
2
2
i

(8)

i

After some algebra, it can be shown that the maximum likelihood estimates of A and B satisfy the
following equations:

�
i

�

ˆ −1 =
Σ
xi

i
−1
ˆ Bx
ˆ i xT
Σ
xi
i

�

ˆ −1 ei eT Σ
ˆ −1
Σ
xi
i
xi

i

=

�

ˆ −1 ei eT Σ
ˆ −1 Bx
ˆ i xT ,
Σ
xi
i
xi
i

i

ˆx = A
ˆ + Bxx
ˆ TB
ˆ T . While not providing closed-form expressions for A
ˆ and B,
ˆ these
where Σ
ˆ −1 that,
equations indicate that the maximum likelihood estimates give a covariance function Σ
xi

loosely speaking, acts “on average” as a pseudo-inverse for
6

ei eTi .

While direct maximization of (8) is challenging, the random eﬀects representation of the model
allows for parameter estimation via simple iterative methods. In particular, maximum likelihood
estimation via the EM algorithm is straightforward, as is Bayesian estimation using a Gibbs sampler to approximate the posterior distribution p(A, B|Y, X). Both of these methods rely on the
conditional distribution of {γ1 , . . . , γn } given {Y, X, A, B}. Straightforward calculations give
{γi |Y, X, A, B} ∼ normal(mi , vi ) , where

vi = (1 + xTi BT A−1 Bxi )−1

mi = vi (yi − µxi )T A−1 Bxi .
Given the variety of modeling options for mean regression, we do not cover estimation of {µx :

x ∈ X } in the next two sections. In what follows we assume {µx : x ∈ X } are known or fixed
at some estimated values. We note that both the EM algorithm and the Gibbs sampling scheme
presented below can be modified to accommodate simultaneous estimation of the mean function.

3.2

Estimation with the EM algorithm

Let ei = (yi − µxi ) and E = (eT1 , . . . , eTn )T . The EM algorithm proceeds by iteratively maximizing
the expected value of the complete data log-likelihood, l(A, B) = log p(E|A, B, X, γ), which is
simply obtained from the multivariate normal density
�
�
n
�
1
T −1
l(A, B) = −
np log(2π) + n log |A| +
(ei − γi Bxi ) A (ei − γi Bxi ) .
2

(9)

i=1

ˆ B)
ˆ of (A, B), one step of the EM algorithm proceeds as follows: First,
Given current estimates (A,
ˆ B,
ˆ ei ] and vi = Var[γi |A,
ˆ B,
ˆ ei ] are computed and plugged into the likelihood (9),
mi = E[γi |A,
giving

ˆ B)]
ˆ = np log(2π) + n log |A| +
−2E[l(A, B)|A,
where

n
�
i=1

ˆ B)]
ˆ
E[(ei − γi Bxi )T A−1 (ei − γi Bxi )|A,

ˆ B)]
ˆ
E[(ei − γi Bxi )T A−1 (ei − γi Bxi )|A,

= (ei − mi Bxi )T A−1 (ei − mi Bxi ) + vi xTi BT A−1 Bxi

= (ei − mi Bxi )T A−1 (ei − mi Bxi ) + si xTi BT A−1 Bxi si ,
˜ is constructed, having ith row equal to mi xi and (n + i)th
with si = vi . Next, a 2n × q matrix X
˜ be the 2n × p matrix given by (ET , 0 × ET )T . The expected
row equal to si xi . Additionally, let E
1/2

value of the complete data log-likelihood can be written as

ˆ B)]
ˆ − np log(2π) = n log |A| + tr([E
˜ − BX][
˜ E
˜ − BX]
˜ T A−1 )
−2E[l(A, B)|A,
7

which is essentially the likelihood for normal multivariate regression. The next step of the EM
ˆ B)
ˆ as the maximizers of this expected likelihood, which are
algorithm obtains the new values (A,
given by
ˆ = E
˜ T X(
˜ X
˜ T X)
˜ −1
B
ˆ = (E
˜ − XB
˜ 1 )T (E
˜ − XB
˜ 1 )/n.
A
This procedure is repeated until a desired convergence criterion has been met.

3.3

Posterior approximation with the Gibbs sampler

A Bayesian analysis provides estimates and confidence intervals for arbitrary functions of the parameters, as well as a simple way of making predictive inference for future observations. Given a
prior distribution p(A, B), inference is based on the joint posterior distribution, p(A, B|Y, X) ∝
p(A, B) × p(Y|X, A, B). While this posterior distribution is not available in closed-form, a Monte

Carlo approximation to the joint posterior distribution of (A, B) is available via Gibbs sampling.
Using the random eﬀects representation of the model in Equation 7, the Gibbs sampler constructs
a Markov chain in {A, B, γ1 , . . . , γn } whose stationary distribution is equal to the joint posterior
distribution of these quantities.

Calculations are facilitated by the use of a semi-conjugate prior distribution for A and B, in
which p(A) is an inverse-Wishart(A−1
0 , ν0 ) distribution having expectation A0 /(ν0 − p − 1) and
p(B|A) is a matrix normal(B0 , A, V0 ) distribution, such that E[B|A] = B0 , E[(B − B0 )(B −
B0 )T )|A] = A × tr(V0 ) and E[(B − B0 )T (B − B0 )|A] = V0 × tr(A). The Gibbs sampler proceeds

by iteratively sampling (A, B) and {γ1 , . . . , γn } from their full conditional distributions. As with
the EM algorithm, we consider inference given values of {µx : x ∈ X }, letting ei = yi − µxi and
E = (e1 , . . . , en )T . One iteration of a Gibbs sampler consists of the following steps:
1. Sample γi ∼ normal(mi , vi ) for each i ∈ {1, . . . , n}, where
vi = (1 + xTi BT A−1 Bxi )−1 ;
mi = vi eTi A−1 Bxi .
2. Sample (A, B) ∼ p(A, B|E, X, γ1 , . . . , γn ) as follows:
(a) sample A ∼ inverse-Wishart(A−1
n , ν0 + n), and

−1
(b) sample B ∼ matrix normal(Bn , A, [XTγ Xγ + V−1
0 ] ), where

Xγ = ΓX, with Γ = diag(γ1 , . . . , γn ),

−1 −1
T
Bn = (ET Xγ + B0 V−1
, and
0 )(Xγ Xγ + V0 )

An = A0 + (E − Xγ Bn )T (E − Xγ Bn ) + (Bn − B0 )T V−1
0 (Bn − B0 ).
8

In the absence of strong prior information, default values for the prior parameters {B0 , V0 , A0 ,
ν0 } can be based on other considerations. In normal regression for example, Zellner [1986] suggests

a “g-prior” which makes the Bayes procedure invariant to linear transformations of the design
matrix X. An analogous result for covariance regression can be obtained by selecting B0 = 0 and
V0 = g(XT X)−1 , i.e. by relating the prior precision of B to the precision given by the observed
design matrix. A typical choice for g is to set g = n so that, roughly speaking, the information
in the prior distribution is equivalent to that contained in one observation. Such choices lead to
what Kass and Wasserman [1995] call a “unit-information” prior distribution, which in some cases
weakly centers the prior distribution around an estimate based on the data. For example, setting
ν0 = p + 2 and A0 equal to the sample covariance matrix of E weakly centers the prior distribution
of A around a “homoscedastic” sample estimate.

3.4

Estimation for higher-rank models

Section 2.3 discussed the possibility of a more flexible covariance regression model by allowing the
deviation between A and Σx to be of a rank greater than one. The general form for a rank-r
covariance regression model is given by
r
�
yi = µxi +
γi,k × B(k) xi + �i
k=1

˜ i ⊗ xi ) + �i , where B
˜ = (B(1) , . . . , B(r) ).
= µxi + B(γ

Estimation for this model can proceed with a small modification of the Gibbs sampling algorithm
given above, in which B(k) and {γi,k , i = 1, . . . , n} are updated for each k ∈ {1, . . . , r} separately.
˜ and {γ 1 , . . . , γ n } are available in closed form,
Alternatively, the full conditional distributions of B
and so the B- and γ-parameters for all ranks could be updated simultaneously. However, in our

experience the calculation of these full conditional distributions is computationally costly: The full
conditional distributions of the γ i ’s involve separate matrix inversions for each i = 1, . . . , n (or
more precisely, for each unique value of the xi ’s). In our experience, sampling the random eﬀects
associated with all ranks simultaneously greatly slows down the Markov chain without providing
improved performance in terms of convergence or mixing.
An EM algorithm is also available for estimation of this general rank model. The main modification to the algorithm presented in Section 3.2 is that the conditional distribution of each γ i is
a multivariate normal distribution, which leads to a more complex E-step in the procedure, while
the M-step is equivalent to a multivariate least-squares regression estimation as before. We note
that, in our experience, convergence of the EM algorithm for ranks greater than 1 can be slow,
presumably due to the identifiability issue described in Section 2.4.
More details about these estimation algorithms for the general rank model are available from
the companion computer code for this article, available at the first author’s website.
9

4

An example with a single continuous predictor

4.1

Heteroscedastic FEV and height data

To illustrate the use of the covariance regression model we analyze data on forced expiratory volume
(FEV) in liters and height in inches of 654 Boston youths [Rosner, 2000]. One feature of these data

75

are the general increase in the variance of these variables with age, as shown in Figure 2.
●

●
●

●

1

●
●
●
●
●
●
●

4

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●

●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●

●
●

●
●
●
●

65

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●

●

●
●
●
●

●
●
●
●
●

height
60

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

●
●
●

●
●

●

●

6

8

10

12

●

●
●

●

45

2

FEV
3

●
●
●

●
●

●

50

4

●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●

55

5

●

70

●

14

16

18

●
●
●
●
●

4

age

●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●
●
●
●
●
●
●
●
●
●
●
●
●
●
●
●

●

●
●
●
●
●
●
●
●
●
●
●
●
●

●

●

●
●
●

●
●
●
●
●

●

●
●

●

●
●
●
●
●
●

●
●

●

●

●
●

●

6

8

10

12

14

16

18

age

Figure 2: FEV and height data, as a function of age. The smooth lines are local polynomial fits.
As the mean responses for these two variables are also increasing with age, one possible modeling
strategy is to apply a variance stabilizing transformation to the data. In general, such transformations presume a particular mean-variance relationship, and choosing an appropriate transformation can be prone to much subjectivity. As an alternative, a covariance regression model allows
heteroscedasticity to be modeled separately from heterogeneity in the mean, and also allows for
modeling on the original scale of the data.

4.2

Maximum likelihood estimation

Ages for the 654 subjects ranged from 3 to 19 years, although there were only two 3-year-olds and
three 19-year-olds. As we will be using plug-in estimates of µx , we combine the data from children
of ages 3 and 19 with those of the 4 and 18-year-olds, respectively, giving a sample size of at least
8 in each age category.
To focus the example on the covariance regression model, we take as our data the bivariate
residuals from two local polynomial regression fits (using loess in the R statistical computing
10

●

0.8

10

Var(FEV)
0.4
0.6

Var(height)
8

●

●

●

●

●

●
●

● ●

●
●

● ●

●
●

●

●

●
●

6

0.5

●

● ●

4

●

4

0.2

●

●

0.6

6

●

●

●

●

●

Cor(FEV,height)
0.7
0.8

1.0

●
●

●

●

●

●

●

● ● ●

0.9

●

12

●

●

8

10 12 14 16 18
age

4

6

8

10 12 14 16 18
age

●

4

6

8

10 12 14 16 18
age

Figure 3: Sample variances and correlations as a function of age, along with covariance regression
fits. The gray lines correspond to a rank-1 model with x = (1, age1/2 ). The black lines correspond
to a rank-2 model with x = (1, age1/2 , age).
environment), one for each of FEV and height. We then use the EM algorithm described in Section
3 to fit the following two covariance regression models:
1/2

Model 1: A rank-1 model with xi = (1, agei );
1/2

Model 2: A rank-2 model with xi = (1, agei , agei ).
Note that including age1/2 as a regressor results in there being a linear component to the modeled
relationship between age and the variances and covariance.
The maximized log likelihoods for these two models are -1927.36 and -1920.49, respectively,
which give the two models roughly the same value of the AIC. However, the increased flexibility of
Model 2 over Model 1 is highlighted in Figure 3, which plots the fitted variances and covariance of
FEV and height as a function of age, along with the sample variances and correlations for each age
group. The plots suggest that the rank-2 model has suﬃcient flexibility to capture the observed
trends in Σx as a function of age.

4.3

Posterior predictive distributions

One potential application of the covariance regression model is to make predictive regions for
multivariate observations. Erroneously assuming a covariance matrix to be constant in x could
give a prediction region with correct coverage probability for an entire population, but incorrect for
specific values of x, and incorrect for making generalizations to populations having a distribution
of x-values that is diﬀerent from that of the data.

11

age 4

age 6

10

age 5

5

●

●
●●
● ●● ●
●
●

0

●● ●
●
●●
●●
●
●● ●
●
●
●
● ●●●
●●
●
● ●
●
●
●
●
●●●
●
●

●
●
●
●
●
● ●
●
●
●
●●
●●●● ●
●
●
● ●●●
●
●●

●

●

−10

●

age 7
10

age 8

5

●

● ●
●● ●
● ●●
● ●
●●
●
●● ●
●
●
●
●
●
●
●
●●
●● ●
●●
●
●
●
●
●
●
●●● ●
●
●
●
●●● ●
● ●
● ● ●
●●●●
●●
● ●● ●
●●
●
●
●
●●●●●●
● ●
●●
●
● ●●
●
●

●●
● ● ●●● ●
●●
●●
●
●●●●●
● ●●
● ●
●
●
●
●
●
●
●
●
●●
●
●● ●
●
●●●● ●
● ●● ●●
● ●

0

●

●

●

−10

●

●
●
●●
●
●
● ●
●● ●
●●●
●● ●
● ●●
●
●
●
● ●●
●
●
●●●
●●
●
●●
●●
●
●
●●●
●●
●● ●
●● ●
●●
●●
●
●●● ●● ●
●●
●●
●
●
●
●●
●
●
●● ●
●
● ●●●
●●
● ●●

age 11

age 12

10

age 10

0

5

●
●● ●
●
●●
●●
●
●●
●
● ●●
●
● ●
●
●●
●●
● ●
●●●
●
●
●
●
●● ●●
● ● ●●●
●
●●●●
●
●
●
●
●●
●
●
●
●● ●
●●● ●
●●●
● ●
● ●
● ●●●
● ●●

●

●

●
●
●
● ●
●●
●
● ●● ●
●●
● ●
●● ● ●●● ●
●●
● ●
●
●
●
●
●● ●
● ●●● ●
●
●● ●●●
●● ●
●●
●
● ●●
●●●●● ●
●
●●
●
●●●
●●
●●
●
●
●
●
●
● ●●
● ●●
●
●●
●

●
● ● ●●
● ●
●
●●● ●
●
●
●
●
●● ●
●●●
● ●
●
●● ●●
●●
●
●
●
●
●●● ●
●●● ●● ● ● ●
●
●
●
●
●

●

●

●

−10

height residual

age 9

●

●

age 14

age 15

10

age 13

5

●
●
●
● ●
●● ●
● ●
●
● ●●
●
● ●●
●●
●● ●
●
●
●
●●
●
●●
●
● ●
●●● ●●●

●
●
●

●●
●●
● ●

● ●
●
●

●

●

●●

●
●

●
●● ● ●
●●
●

●
●
●

−10

0

●

●
● ●●
●

●
●●

●
●●

●

●

age 17

age 18

10

age 16

5

●
●

●

●

0

●
●
●

●

●
●

●

●

● ●

●
●

●

●

●
●

●
●

●●

●
●

●●

●

−10

●

−2

−1

0

1

2

−2

−1

0

1

2

−2

−1

0

1

2

FEV residual

Figure 4: Observed data and 90% posterior predictive ellipsoids for each age. The black ellipsoids
correspond to the covariance regression model, and the gray to a model with constant variance.

12

age group
4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

sample size

11

28

37

54

85

94

81

90

57

43

25

19

13

8

9

homoscedastic

1

.96

.97

.96

.96

.95

.95

.89

.77

.84

.76

.79

.85

.75

.78

heteroscedastic

1

.86

.92

.91

.88

.93

.95

.89

.89

.88

.88

.89

.92

.88

.89

Table 1: Observed-data coverage rates by age for the heteroscedastic predictive ellipse from the
covariance regression model, and the homoscedastic predictive ellipse from a constant covariance
model. The nominal (target) coverage rates for the ellipses is 90%.
Predictive inference is straightforward to implement in the context of Bayesian estimation: The
prior distributions and data generate a predictive distribution p(˜
y|˜
x, Y, X) for each possible value

˜ , which can be approximated via the output from the Markov chain Monte Carlo algorithm
of x
described in Section 3. Using Model 2 described above and the default prior distributions discussed in Section 3, 50,000 iterations of the Gibbs sampler were generated, the first 1,000 of which
were discarded to allow for convergence to the stationary distribution. Parameter values were
saved every 10th iteration thereafter, leaving 4,900 saved values with which to make Monte Carlo
approximations.
For each of the 4,900 generated values of {A, B}, we constructed Σx (A, B) for each age from 4 to

18, yielding 45 parameters for each value of {A, B}. Eﬀective sample sizes (roughly, the equivalent

number of independent Monte Carlo samples) for these 45 parameters were all above 1000, with
the exception of σ12 and σ1,2 for the 18-year-old age group, which had eﬀective sample sizes of 988
and 713 respectively. For each age group x and each of the 4,900 values of Σx , a predictive sample
˜ was generated from the multivariate normal(0, Σx ) distribution. A 90% predictive ellipse was
y
˜ -values
then generated as the smallest ellipse that contained 90% of the 4,900 posterior predictive y
for the given age group. These ellipses are displayed graphically in Figure 4, along with the data
and an analogous predictive ellipse based on a homoscedastic (constant covariance) model.
Averaged across observations from all age groups, both of the two sets of ellipsoids contain
90.5% of the observed data, which is very close to the nominal coverage of 90%. However, as can
be seen from Table 1, the homoscedastic ellipse overcovers the observed data for the younger age
groups, and undercovers for the older groups. In contrast, the flexibility of the covariance regression
model allows the confidence ellipsoids to change size and shape as a function of age, and thus is
able to match the nominal coverage rate fairly closely across the diﬀerent ages.

5

Discussion

This article has presented a model for a covariance matrix Cov[y|x] = Σx as a function of an
explanatory variable x. We have presented a geometric interpretation in terms of curves along the
13

boundary of a translated positive definite cone, and have provided a random eﬀects representation
that facilitates parameter estimation. This covariance regression model goes beyond what can be
provided by variance stabilizing transformations, which serve to reduce the relationship between
the mean and the variance. Unlike models or methods which accommodate heteroscedasticity in
the form of a mean-variance relationship, the covariance regression model allows the mean function
µx to be parameterized separately from the variance function Σx .
Although the example in this article involved a single continuous predictor, the covariance
regression model accommodates explanatory variables of all types, including categorical variables.
This could be useful in the analysis of multivariate data sampled from a large number of groups, such
as groups are defined by the cross-classification of several categorical variables. For example, it may
be desirable to estimate a separate covariance matrix for each combination of age group, education
level, race and religion in a given population. The number of observations for each combination of
explanatory variables may be quite small, making it impractical to estimate a separate covariance
matrix for each group. A practical alternative would be to use a covariance regression model as a
parsimonious representation of the heteroscedasticity across the groups.
Like mean regression, a challenge for covariance regression modeling is variable selection, i.e. the
choice of an appropriate set of explanatory variables. One possibility is to use selection criteria such
as AIC or BIC, although non-identifiability of some parameters in the higher-rank models requires
a careful accounting of the number of parameters. Another possibility may be to use Bayesian
procedures, either by Markov chain Monte Carlo approximations to Bayes factors, or by explicitly
formulating a prior distribution to allow some coeﬃcients to be zero with non-zero probability.
Example code and an R-package for the EM and Gibbs sampling algorithms are available at
the first author’s website: www.stat.washington.edu/~hoff

References
G. E. P. Box and D. R. Cox. An analysis of transformations. (With discussion). J. Roy. Statist.
Soc. Ser. B, 26:211–252, 1964. ISSN 0035-9246.
Raymond J. Carroll. Adapting for heteroscedasticity in linear models. Ann. Statist., 10(4):1224–
1233, 1982. ISSN 0090-5364. URL http://links.jstor.org/sici?sici=0090-5364(198212)
10:4<1224:AFHILM>2.0.CO;2-H&origin=MSN.
Raymond J. Carroll, David Ruppert, and Robert N. Holt, Jr. Some aspects of estimation in
heteroscedastic linear models. In Statistical decision theory and related topics, III, Vol. 1 (West
Lafayette, Ind., 1981), pages 231–241. Academic Press, New York, 1982.
Robert F. Engle and Kenneth F. Kroner. Multivariate simultaneous generalized arch. Econometric
14

Theory, 11(1):122–150, 1995. ISSN 0266-4666. doi: 10.1017/S0266466600009063. URL http:
//dx.doi.org.offcampus.lib.washington.edu/10.1017/S0266466600009063.
P. W. Fong, W. K. Li, and Hong-Zhi An. A simple multivariate ARCH model specified by
random coeﬃcients. Comput. Statist. Data Anal., 51(3):1779–1802, 2006. ISSN 0167-9473.
doi: 10.1016/j.csda.2005.11.019. URL http://dx.doi.org.offcampus.lib.washington.edu/
10.1016/j.csda.2005.11.019.
Robert E. Kass and Larry Wasserman. A reference Bayesian test for nested hypotheses and its
relationship to the Schwarz criterion. J. Amer. Statist. Assoc., 90(431):928–934, 1995. ISSN
0162-1459.
Hans-Georg M¨
uller and Ulrich Stadtm¨
uller. Estimation of heteroscedasticity in regression analysis.
Ann. Statist., 15(2):610–625, 1987. ISSN 0090-5364. doi: 10.1214/aos/1176350364. URL http:
//dx.doi.org/10.1214/aos/1176350364.
Bernard Rosner. Fundamentals of Biostatistics. Duxbury Press, 2000. ISBN 0-534-37068-3.
Herbert C. Rutemiller and David A. Bowers. Estimation in a heteroscedastic regression model. J.
Amer. Statist. Assoc., 63:552–557, 1968. ISSN 0162-1459.
M.A. Scott and M.S. Handcock. Covariance Models for Latent Structure in Longitudinal Data.
Sociological Methodology, pages 265–303, 2001.
Gordon K. Smyth.

Generalized linear models with varying dispersion.

Ser. B, 51(1):47–60, 1989.

ISSN 0035-9246.

J. Roy. Statist. Soc.

URL http://links.jstor.org/sici?sici=

0035-9246(1989)51:1<47:GLMWVD>2.0.CO;2-4&origin=MSN.
Arnold Zellner. On assessing prior distributions and Bayesian regression analysis with g-prior distributions. In Bayesian inference and decision techniques, volume 6 of Stud. Bayesian Econometrics
Statist., pages 233–243. North-Holland, Amsterdam, 1986.

15

