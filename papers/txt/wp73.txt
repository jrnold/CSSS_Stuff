Split-Sample Score Tests in Linear Instrumental Variables
Regression ∗
Saraswata Chaudhuri, Thomas Richardson, James Robins and Eric Zivot

Working Paper no. 73
Center for Statistics and the Social Sciences
University of Washington
March 30, 2007

Saraswata Chaudhuri is a graduate student of Economics, University of Washington, Box 353330, Seattle WA
98195. E-mail: saraswat@u.washington.edu. Thomas Richardson is Associate Professor of Statistics, Department
of Statistics, University of Washington. James Robins is Professor of Epidemiology and Biostatistics, Departments
of Epidemiology and Biostatistics, Harvard University. Eric Zivot is Associate Professor of Economics, Deparment of
Economics, University of Washington. Research in this paper was supported by: the Gary Waterman Distinguished
Scholar Fund; a seed grant from the Center for Statistics and the Social Sciences at the University of Washington
(PI:Zivot); NSF grant DMS 0505865 (PI: Richardson); NIH grant R01 AI032475 (PI:Robins). Saraswata Chaudhuri
thanks Professors Charles R. Nelson, Richard Parks, Michael D. Perlman, Elaina Rose and Richard Startz and
the seminar participants at the University of Washington (Economics Department) for their helpful comments and
suggestions.
∗

i

Abstract
In this paper we design two split-sample tests for subsets of structural coefficients in a linear
Instrumental Variables (IV) regression. Sample splitting serves two purposes – 1) validity
of the resultant tests does not depend on the identifiability of the coefficients being tested
and 2) it combines information from two unrelated samples one of which need not contain
information on the dependent variable. The tests are performed on sub-sample one using
the regression coefficients obtained from running the so-called first stage regression on subsample two (sample not containing information on the dependent variable). The first test uses
the unbiased split-sample IV estimator of the remaining structural coefficients constrained
by the hypothesized value of the structural coefficients of interest [see Angrist and Krueger
(1995)]. We call this the USSIV score test. The USSIV score test is asymptotically equivalent
to the standard score test based on sub-sample one when the standard regularity conditions
are satisfied. However, the USSIV score test can be over-sized if the remaining structural
coefficients are not identified. This motivates another test based on Robins (2004), which
we call the Robins-test. The Robins-test is never oversized and if the remaining structural
coefficients are identified, the Robins-test is asymptotically equivalent to USSIV score test
√
against n-local alternatives.
KEY WORDS:

Instrumental Variables; Sample Splitting.

1

1

Introduction

In this paper we propose a new split-sample score test for subsets of structural coefficients in linear
Instrumental Variables (IV) models and show that it is valid irrespective of the relevance of the
instruments. Our test is quite generally less conservative than the projection test based on the
split-sample (score) statistic proposed by Dufour and Jasiak (2001).
Split-sample methods of estimating structural coefficients in linear IV models were proposed by
Angrist and Krueger (1995) to avoid biased estimation in the presence of irrelevant instruments.
The estimation procedure typically involves three steps – 1) split the sample randomly into two subsamples, 2) compute the predicted values of the endogenous regressors by multiplying the values of
the instruments in sub-sample one with the coefficients obtained by running the so-called first-stage
regression on sub-sample two and 3) use sub-sample one to estimate the structural coefficients by
running a regression of the dependent variable on the predicted values of the endogenous regressors.
Dufour and Jasiak (2001) show that a split-sample version of the score test [see Wang and Zivot
(1998)] can be used to jointly test all the structural coefficients in a linear IV regression. Simulation
results from Kleibergen (2002) show that when the degree of over-identification is large, split-sample
score test can be more powerful than the Anderson-Rubin (AR) test based on the whole sample
[see Dufour (1997) and Staiger and Stock (1997)].
A common argument against sample-splitting is the wastage of information which results in loss
of power (efficiency). For example, the split-sample score test for jointly testing all the structural
coefficients is less powerful than other similar tests like the K-test [see Kleibergen (2002)] and the
Conditional Likelihood Ratio (CLR) test [see Moreira (2003)] based on the whole sample. But there
is another way to look at it – with randomly missing data, the split-sample score test can actually
use more information than the K-test or the CLR test mentioned above. For example, consider the
familiar IV regression model:
y = Yθ+u
Y

= ZΠ + V

where u and V are the unobserved correlated structural errors, Y is the set of endogenous regressors
and Z is the set of instrumental variables. Let θ be the parameter of interest and suppose that all
the regularity conditions are satisfied. Also, suppose that we have n independent and identically distributed observations on Y and Z, but only n1 < n observations on y where n2 = n−n1 observations
of y are randomly missing.
1

1

In this situation the K-test and the CLR-test, in their present form,

If we think of each observation on y being included independently in the sample with probability p, then by

2

can use only n1 of the observations on all the variables and the remaining n2 observations on Y and
Z are wasted, whereas the split-sample score test actually uses all the information available. See
Angrist and Krueger (1992) for another example where sample splitting (in other words, using two
different samples) is a reasonable option. The structure of the split-sample score test does not allow
for a gain in power under the above situation. However, Monte-Carlo experiments (not reported
in this paper) show that if a majority of the observations on y are missing, the match between the
nominal and the asymptotic size is possibly better for the split-sample score test than it is for the
AR-test, the K-test or the CLR-test based on sub-sample one alone (i.e. the complete observations).
The present paper extends the split-sample score test to subsets of parameters. Suppose that in
the IV model described above, θ = (β ′ , γ ′ )′ and Y = (X, W ), i.e. β is the coefficient associated
with the endogenous regressor X and γ is the coefficient associated with the other endogenous
regressor W . For example, if we are interested in the returns to schooling, X can be considered
as the years of schooling, W as the years of experience and y as logarithm of the wage earned by
the individual. Both X and W presumably depend on the unobserved ability of the individual
which is probably correlated with the individual’s earning, and hence X and W can be argued to
be endogenous. For simplicity, we do not mention the exogenous control variables included in the
regression. As mentioned above, β is the coefficient of interest. If the instruments Z are “weak” for
β, we cannot estimate β consistently but it is still possible to validly test null hypotheses of the form
H0 : β = β0 using projection test based on the AR-statistic or the K-statistic or the LR-statistic
[see Staiger and Stock (1997), Kleibergen (2002) and Moreira (2003)]. 2 However, projection tests
of H0 : β = β0 can be very conservative when the instruments are “strong” for either γ or β.
Kleibergen (2004) proposes an alternative way of using the K-statistic to obtain a valid test of
H0 : β = β0 when the instruments are strong for γ; Zivot et al. (2006) call this the “partial” K-test.
The partial K-statistic replaces γ by its limited-information maximum likelihood (LIML) estimator
(constrained by H0 : β = β0 ) in the K-statistic and adjusts the degree of freedom of the asymptotic
distribution accordingly. The partial K-test is more powerful than the projection test based on the
K-statistic and the partial K-statistic is pivotal when the instruments are strong for γ. Given that
strong instruments are not easy to find, it is not advisable to simply assume that the instruments
are strong for γ. Hence a reliable use of the partial K-test must be accompanied by a pretest for
the strength of instruments for γ and possibly with proper size-correction thereafter [see Hall et al.
“randomly missing observations” we mean that p is determined exogenously.
2
In the rest of the paper “valid tests” is synonymous to “tests that are not over-sized”. If the instruments
are “weak” for any structural coefficient in the sense of Staiger and Stock (1997), then the coefficient is asymptotically unidentified. However, strictly speaking, “strong” instrument for any structural coefficient in the sense of
Staiger and Stock (1997) does not necessarily mean that the parameter is identified [see WI Case 3 in Zivot et al.
(2006)]. In the present paper we do not consider this case and use the term “strong” instrument loosely to mean
that the corresponding structural coefficient is identified.

3

(1996)]. 3 However, similar to Guggenberger and Smith (2005) and Zivot et al. (2006), we do not
find any evidence of major upward-size distortion of the partial K-test but we do observe that the
size of the partial K-test increases with the level of endogeneity of W (i.e. the regressor associated
with the structural coefficient γ).
In the present paper we propose a new test, the Robins-test, which is valid irrespective of the
strength of the instruments, for either β or γ, under weak assumptions about the underlying data
generating process. When all the standard regularity conditions are satisfied, the power of this test
is asymptotically equivalent to the power of the standard score test (based on sub-sample one).
When the rank condition is not satisfied, the standard score test is invalid, whereas, our method
can still be relaibly used for inference on subsets of structural coefficients. We also show that our
method is computationally easy to implement and we recommend its use for testing subsets of
structural coefficients in linear IV models. As shown by Zivot et al. (1998), we can also form a
confidence interval (of any desired level and probably conservative) for β as the collection of values
β0 which cannot be rejected by our test (at the desired level). Again, if all the standard regularity
conditions are satisfied, any point belonging to such a confidence interval (of non-trivial level) will
√
belong to the n-neighborhood of the true value of the structural coefficients with probablity one.
So like the standard tests, our method can also be used for consistent interval-estimation of subsets
of structural coefficients when the standard regularity conditions are satisfied.
The main contributions of this paper are summarized below:
• First we propose a split-sample score test for the null hypothesis H0 : β = β0 which is valid
when the instruments are strong for γ. Let b
γ (β0 ) be the Unbiased-Split-Sample-InstrumentalVariables (USSIV) estimator of γ constrained by the null hypothesis H0 : β = β0 [see
Angrist and Krueger (1995)]. We define a test which rejects the null hypothesis at level α if

the split-sample score statistic evaluated at β = β0 and γ = b
γ (β0 ) exceeds the (1 − α)100-th
quantile of the central χ2 distribution with degrees of freedom equal to the dimension of β
[see Dufour and Jasiak (2001)]. We call this the USSIV score test. When the instruments are
strong for γ, the USSIV score test for β is valid and asymptotically equivalent to the partial
√
K-test (based on sub-sample one) against n-local alternatives. When the instruments are
weak for γ, we do not have control over the size of the USSIV score test. The USSIV score test
serves the purpose of a benchmark for split-sample score tests (or tests based on sub-sample
one) of H0 : β = β0 when the instruments are strong for γ.
• Finally, we design a new testing procedure for the null hypothesis H0 : β = β0 which is
3

However, to our knowledge, there does not exist any systematic way of pretesting the strength of instruments
in the presence of multiple endogenous regressors. A widely used (abused) measure of instrumental strength is the
partial R2 statistic proposed by Shea (1997). Zivot and Chaudhuri (2007) point out that the partial R2 statistic for
γ, in this case, can be totally misleading if the instruments are weak for β.

4

always valid. When the instruments are strong for γ, it is asymptotically equivalent to the
USSIV score test. To our knowledge this kind of test was first proposed by Robins (2004) and
hence we call it the Robins-test. The Robins-test is quite generally more powerful than the
projection test based on the split-sample score statistic proposed by Dufour and Jasiak (2001).

The rest of the paper is organized as follows: Section 2 discusses the model and the weak instrument
framework, Section 3 describes testing of hypotheses on subsets of parameters using the USSIV score
test and Robins-test, Section 4 is a Monte-Carlo study investigating the finite sample behaviors of
the USSIV score test and Robins-test, we conclude in Section 5.
We use the following notations throughout. Consider any n × m matrix A. If A has full column

rank then PA = A(A′ A)−1 A′ and MA = In − PA . If A is a symmetric, positive semidefinite matrix
1′
1
1
then A = A 2 A 2 where A 2 is the lower-triangular Cholesky factor of A.

2

Linear IV Model with Weak Instruments

2.1

Model:

Suppose that we have the following structural equation model:
y = Xβ + W γ + u

(1)

X = ZΠX + VX

(2)

W = ZΠW + VW

(3)

where y is the dependent variable, X and W are the endogenous regressors, Z is the instrument
and u, VX and VW are the unobserved correlated structural errors. 4 Let the dimensions of β, γ,
ΠX and ΠW be respectively mx × 1, mw × 1, k × mx and k × mw . Let m = mx + mw and m, mx , mw
and k be fixed and finite numbers. We assume that the order condition k ≥ m is satisfied. We do
not, however, impose the restriction of full column rank on Π = [ΠX , ΠW ].
Suppose that we have n observations on y, X, W and Z and we randomly split the sample in two
parts – the first part containing n1 = [nζ] observations and the second part containing n2 = n − n1
observations where ζ is a fixed number in the interval (0, 1). Let yi , Xi , Wi and Zi represent the
ni observations in the ith sub-sample (i = 1, 2) where the observations are stacked in rows. To
4

For simplicity, we leave out the included exogenous variables. √Adding them in the model does not entail any
fundamental change in our results because it is possible to find n-consistent estimators for the corresponding
coefficients when the true values of β and γ are known.

5

motivate alternatively, we can also assume that y2 is missing and hence the AR-test, the K-test
and the CLR test, in their present form, can only be performed based on the first sub-sample. The
asymptotic results discussed in this paper do not depend on whichever motivation is used – sample
splitting or missing y2 . However, for the purpose of the simulations we use the missing y2 scanario.
We make the following high level assumptions summarized under Assumption A.
Assumption A: We assume that the following results hold jointly as n → ∞ for i = 1, 2:


 σuu σuX σuW 


P

→Σ=
1. n1i (ui, VXi , VW i)′ (ui , VXi , VW i) −
σ
σ
σ
XX
XW  where Σ is a symmetric, posi Xu


σW u σW X σW W
tive definite matrix.
2.

P
1
Z ′Z −
→
ni i i

3.

√1 Zi′
ni

Q where Q is a symmetric, positive definite matrix.
d

1

(ui, VXi , VW i) −
→ Q 2 (ΨZui, ΨZXi, ΨZW i) where vec (ΨZui, ΨZXi , ΨZW i) ∼ N(0, Σ ⊗ Ik ).

4. Finally, we assume that ΨZu1 , ΨZX1, ΨZW 1 are uncorrelated with ΨZX2 and ΨZW 2 .

See Staiger and Stock (1997) and Kleibergen (2002) for discussions on the first three assumptions.
The fourth assumption ensures that the random functions computed from sub-sample one are
asymptotically uncorrelated with those computed from sub-sample two.

2.2

Weak Instrument Framework:


The weak-instrument framework proposed by Staiger and Stock (1997) models Π as O nδ where

δ = 0 means that the instruments are strong and δ = − 21 means that they are weak for the
corresponding structural coefficients. The framework ties the instrument-strength to the sample size
in a way which ensures that as the sample size goes to infinity the instruments become completely
irrelevant at a rate that gives non-trivial asymptotic properties. We use the weak-instrument
framework to characterize four cases of partial identification under Assumption A [see Zivot et al.
(2006), Choi and Phillips (1992) and Phillips (1989)] as given below:
C
√W
n

• Case 1: ΠX =

C
√X
n

and ΠW =

• Case 2: ΠX =

C
√X
n

and ΠW = CW i.e. β is asymptotically unidentified but γ is identified.

• Case 3: ΠX = CX and ΠW =

C
√W
n

i.e. both β and γ are asymptotically unidentified.

i.e. β is identified but γ is asymptotically unidentified.

• Case 4: ΠX = CX and ΠW = CW i.e. both β and γ are identified.
6

where C = [CX , CW ] is a full-column-rank, non-random matrix with fixed and bounded elements.
This is by no means an exhaustive list of all possible cases, but is sufficiently rich to highlight the
interesting asymptotic results.
1

We also define Λ = (λX , λW ) = Q 2 C and note that under the special case λ′X λW = 0 and σXW = 0,
Zivot et al. (2006) define the so-called concentration parameter (measuring the instrument strength)
for β and γ respectively as
µβ =

n(1+2δx ) − 12 ′ ′
− 21
σXX λX λX σXX
k

and µγ =

n(1+2δw ) − 12 ′ ′
−1
σW W λW λW σW2W
k

where δx = − 12 under Cases 1 and 2 and δx = 0 under Cases 3 and 4. Similarly δw = − 12 under
Cases 1 and 3 and δw = 0 under Cases 2 and 4. However, in the rest of the paper we do not impose
the restrictions λ′X λW = 0 and σXW = 0 (except in Section 4: simulation study).

3

Testing the Null Hypothesis on Subsets of Parameters

Without loss of generality we treat β as the parameters of interest and γ as the nuisance parameters.

3.1

Why Split the Sample?

We follow the exposition by Wang and Zivot (1998) to motivate sample-splitting (or equivalently
using information from both sub-samples). The Wang and Zivot score statistic for jointly testing
β = β0 and γ = γ0 , based on sub-sample one, uses the gradients of the following objective function
with respect to β and γ
1
max J(β, γ) = − (y1 − X1 β − W1 γ)′ PZ1 (y1 − X1 β − W1 γ).
β,γ
2
The gradients with respect to β and γ are, respectively, given by
b ′ Z ′ (y1 − X1 β − W1 γ)
▽β J11 (β, γ) = Π
X1 1
′
b Z ′ (y1 − X1 β − W1 γ)
▽γ J11 (β, γ) = Π
W1

1

(4)
(5)

b Xi = (Z ′ Zi )−1 Z ′ Xi and Π
b W i = (Z ′ Zi )−1 Z ′ Wi are the first-stage estimators of ΠX and ΠW
where Π
i
i
i
i

based on sub-sample i (= 1,2). The subscript on ▽J is used to distinguish (4) and (5) from (8) and
(9) respectively.

7

h
i
′
′
′
bi = Π
b Xi , Π
b W i for i = 1, 2. The Wang and Zivot
Let ▽J11 (β, γ) = [▽β J11
(β, γ), ▽γ J11
(β, γ)] and Π
score statistic is defined as
WZ(β0 , γ0 ) =

′
▽J11
(β0 , γ0 )
1
(y
n1 1


−1
′ ′
b
b
Π1 Z1 Z1 Π1
▽J11 (β0 , γ0 )

(6)

− X1 β0 − W1 γ0 )′ (y1 − X1 β0 − W1 γ0 )

and the score test rejects the hypotheses β = β0 and γ = γ0 jointly at level α if WZ(β0 , γ0 ) > χ2m (1−
α) where χ2m (1 − α) is the (1 − α)100-th quantile of the central χ2m distribution. Wang and Zivot

(1998) point out that under Case I, if β0 and γ0 are the true values, then
d

WZ(β0 , γ0 ) −
→

1 ′
Ψ P[λ +Ψ ,λ +Ψ ] ΨZu1
σuu Zu1 X ZX1 W ZW 1

which is not a (central) χ2m distribution because ΨZu1 is correlated with ΨZX1 and ΨZW 1 . The
b 1 , used in the expression of ▽J11 (β, γ), are
problem arises because √1n1 Z1′ (y1 − X1 β − W1 γ) and Π

asymptotically correlated in the presence of weak instruments.

b 1 by Π
b 2 in (6) and obtain the split-sample score statistic
Dufour and Jasiak (2001) replace Π
LM(β0 , γ0 ) =

′
▽J21
(β0 , γ0)

1
(y
n1 1


−1
′ ′
b
b
Π2 Z1 Z1 Π2
▽J21 (β0 , γ0 )

− X1 β0 − W1 γ0 )′ (y1 − X1 β0 − W1 γ0 )

5

(7)

′

′
′
where ▽J21 (β, γ) = [▽β J21
(β, γ), ▽γ J21
(β, γ)] denotes the new gradient vector using information
from both the sub-samples and

b ′ Z ′ (y1 − X1 β − W1 γ)
▽β J21 (β, γ) = Π
X2 1
′
b
▽γ J21 (β, γ) = Π Z ′ (y1 − X1 β − W1 γ)
W2

1

(8)
(9)

It is probably more appropriate to call (8) and (9) the “pseudo-gradients” with respect to β and γ
respectively. When the instruments are strong, the pseudo-gradients are asymptotically equivalent
to the gradients given in (4) and (5).
d

When β = β0 and γ = γ0 , LM(β0 , γ0 ) −
→ χ2m . The split-sample score test for jointly testing β = β0
and γ = γ0 rejects the hypotheses at level α if LM(β0 , γ0 ) > χ2m (1−α). The test is valid irrespective
5

This is also referred to as the split-sample Anderson-Rubin statistic by Staiger and Stock (1997), as the splitsample statistic by Dufour and Taamouti (2005) and Kleibergen (2002). However, we use √
the expression of the
denominator of the statistic from Wang and Zivot (1998). Since we will restrict ourselves to n-local alternatives,
the choice of denominator is not going to matter asymptotically.

8

of the strength of the instruments.
In a separate Monte-Carlo study (not reported in this paper) we compare the finite sample behavior of the (joint) split-sample score test, the AR-test, the K-test and the CLR-test (based on
sub-sample one) for different levels of instrument relevance and endogeneity. When the number
of observations in sub-sample one is small, the nominal sizes of the AR-test, the K-test and the
CLR-test exceed the asymptotic size. On the other hand, the split-sample score test has correct
nominal size even when the number of observations in sub-sample one is small. This is probably not
surprising given that the two sub-samples are actually drawn independently under our Monte-Carlo
design. However, the fact that the split-sample score test uses more information than the AR, the
K and the CLR tests, based on sub-sample one only, does not imply that it is more powerful in
finite samples. Asymptotically these tests (except the AR test in the over-identified case) have the
√
same power against n-local alternatives when the instruments are strong.
In the following sub-sections we exploit the (asymptotic) absence of correlation between the two
sub-samples to design split-sample score tests for subsets of structural coefficients i.e. for the null
hypothesis H0 : β = β0 .

3.2

USSIV Score Test:

Given any value β0 , the USSIV estimator of γ, as defined in Angrist and Krueger (1995), is obtained
from (9) by solving ▽γ J21 (β0 , γ) = 0 and is given by

−1
b ′ Z ′ W1
b ′ Z ′ (y1 − X1 β0 )
γb(β0 ) = Π
Π
W2 1
W2 1

(10)

Replacing γ0 by γb(β0 ) in (7) we get what we call the USSIV-score statistic. The USSIV score
statistic is given by

LMβ (β0 ) =

=

′
▽J21


−1
′ ′
b
b
(β0 , b
γ (β0 )) Π2 Z1 Z1 Π2
▽J21 (β0 , b
γ (β0 ))

(y1 − X1 β0 − W1 b
γ (β0 ))′ (y1 − X1 β0 − W1 b
γ (β0 ))

−1
′
b ′ Z ′ M b Z1 Π
b X2
▽β J21
(β0 , b
γ (β0 )) Π
▽β J21 (β0 , b
γ (β0 ))
X2 1 Z1 ΠW 2
1
n1

1
n1

(y1 − X1 β0 − W1 b
γ (β0 ))′ (y1 − X1 β0 − W1 b
γ (β0 ))

(11)

where (11) is obtained by using the definition of the USSIV estimator of γ given β0 . The USSIV
score test rejects the null hypothesis H0 : β = β0 at level α if LMβ (β0 ) > χ2mx (1 − α).
Theorem 1: When n → ∞ and β = β0 +

d
√β ,
n

9

the asymptotic distribution of the USSIV score

statistic under Cases 2 and 4 is given by
d

• In Case 2: LMβ (β0 ) −
→ χ2mx .
 ′ ′

ζdβ λX PMλW λX λX dβ
d
2
.
• In Case 4: LMβ (β0 ) −
→ χmx
σuu
We prove Theorem 1 in the Appendix. Theorem 1 says that in Case 2, where the instruments are
√
weak for β but strong for γ, the USSIV score test cannot distinguish the true β from the n-local
alternatives. In Case 4, where the instruments are strong for both β and γ, the USSIV score test
behaves like the standard score test for a subset of structural coefficients in a linear IV model
using the whole sample – the only difference being the non-centrality parameter of the asymptotic
distribution which is a fraction ζ of the non-centrality parameter of the asymptotic distribution of
the standard score statistic based on the whole sample. Theorem 1 does not tell us anything about
Cases 1 and 3 where the instruments are weak for γ. Noting that under Cases 1 and 3,
d

γ (β0 ) − γ −
b
→



λW

p

1 − ζ + ΨZW 2

′ 

λW

p

ζ + ΨZW 1

−1 

λW

p

1 − ζ + ΨZW 2

′

ΨZu1 > op (1)

when H0 : β = β0 is true, it can be shown that the asymptotic distribution of the USSIV score
statistic under H0 : β = β0 does not follow χ2mx unless the regressor W in (1) is (weakly) exogenous.
Similarity between the asymptotic behaviors of the USSIV test and the partial K-test in Cases 2
and 4 follow from Theorem 1.
Now we define a new statistic which helps to explore the asymptotic relationship between the USSIV
score test discussed in this section and Robins-test discussed in the next section. Consider any γ∗
√
in the n-neighborhood of the true γ i.e. let γ = γ∗ + √dγn . Define LMβ∗ (β0 , γ∗ ) as

LMβ∗ (β0 , γ∗ )

=

(y1 − X1 β0 − W1 γ∗ )′ PhM
1
n1

b

Z1 ΠX2
b
Z1 Π
W2

i (y

1

− X1 β0 − W1 γ∗ )

(y1 − X1 β0 − W1 γ∗ )′ (y1 − X1 β0 − W1 γ∗ )

.

(12)

It is easier to motivate the two statistics LMβ (β0 ) and LMβ∗ (β0 , γ∗ ) if we think of the standard IV

regression without any weak instrument i.e. when sample splitting is not necessary. In this setting
the standard score test based on sub-sample one rejects H0 : β = β0 at level α if

WZ β (β0 ) =

(y1 − X1 β0 − W1 b
γ2SLS (β0 ))′ PhM

b
Z1 Π
W1

b X1
Z1 Π

2SLS
σ
buu

i (y

1

− X1 β0 − W1 b
γ2SLS (β0 ))

> χ2mx (1 − α)

(13)

−1
b ′1 Z1′ Z1 Π
b1
b ′1 Z1′ (y1 −X1 β0 ) is the constrained two-stage least squares (2SLS)
where b
γ2SLS (β0 ) = Π
Π
estimator of γ satisfying ▽γ J11 (β0 , b
γ2SLS (β0 )) = 0 and and the estimator of the residual variance
10

′

2SLS
under the null hypothesis is σ
buu
= (y1 −X1 β0 −W1 γb2SLS (β0 ))n1(y1 −X1 β0 −W1 bγ2SLS (β0 )) . In the context of
maximum likelihood (for example, if the structural errors jointly follow Gaussian distribution), we

−1
2SLS b ′
b X1
can think of the term σ
buu
ΠX1 Z1′ MZ1 Πb W 1 Z1 Π
as the top-left mx × mx block in the inverse

of the (Hessian-based) estimator of the Information Matrix. i.e. as the estimator of the information
bound for estimating β. Equivalently, it is the inverse of the variance estimator of the efficient score
√
function for β evaluated at the ( n-neighborhood of the) true values of β and γ. If we define the
efficient score statistic for β as the quadratic form of the sample efficient score function with respect

to the inverse of its estimated variance, then the efficient score statistic, based on sub-sample one,
and evaluated at β0 and γ∗ is given by
WZ ∗β (β0 )

=

(y1 − X1 β0 − W1 γ∗ )′ PhM
1
n1

b
Z1 Π
W1

b X1
Z1 Π

i (y

1

− X1 β0 − W1 γ∗ )

(y1 − X1 β0 − W1 γ∗ )′ (y1 − X1 β0 − W1 γ∗ )

.

(14)

b 1 by Π
b 2 in the expression of
The USSIV score statistic given in (11) is obtained by replacing Π
W Zβ (β0 ) in (13) and the USSIV estimator of γ, (i.e. b
γ (β0 ) in (10)) can be thought of as the

split-sample version of the 2SLS estimator of γ. Similarly the statistic LMβ∗ (β0 , γ∗ ) given in (12)
can be thought of as a split-sample version of the efficient score statistic W Zβ∗ (β0 , γ∗) given in (14).
Lemma 1 discusses the asymptotic properties of LMβ∗ (β0 , γ∗ ) which are used to prove Theorem 1.

It also helps to motivate the new split-sample score test introduced in the next section.

Lemma 1: Let 1[δx =0] be a dummy variable taking value one if δx = 0 and taking value zero

if δx = − 21 where δx is such that ΠX = O nδx . Similarly define the dummy variable 1[δw =0]

d
where δw is such that ΠW = O nδw . Suppose that β = β0 + √βn and γ = γ∗ + √dγn . Also, let B =

 p


√
M[λW √1−ζ+(1−1δ =0 )ΨZW 2 ] λX 1 − ζ + (1 − 1δx =0 )ΨZX2 and D = ΨZu1 + ζ 1[δx =0] λX dβ + 1[δw =0] λW dγ .
w
When n → ∞ the asymptotic distribution of LMβ∗ (β0 , γ∗ ) is given by
d

LMβ∗ (β0 , γ∗ ) −
→

D′ P B D
σuu

(15)

We prove Lemma 1 in the Appendix. Several points are worth mentioning here.
1. The asymptotic distribution of LMβ∗ (β, γ∗ ) is central χ2mx , i.e. at the true value of β the
asymptotic distribution does not depend on the strength of the instruments.
2. Under Case 2, the asymptotic distribution of LMβ∗ (β0 , γ∗ ) is again central χ2mx .
3. Under Case 4, the asymptotic distribution of LMβ∗ (β0 , γ∗) is non-central χ2mx with nonζd′β λ′X PMλW λX λX dβ
.
centrality parameter given by
σuu

11

The above observations suggest asymptotic equivalence between LMβ (β0 ) and LMβ∗ (β0 , γ∗) when
instruments are strong for γ. See the proof of Theorem 1 for a formal statement of asymptotic
equivalence between LMβ (β0 ) and LMβ∗ (β0 , γ∗ ) under cases 2 and 4.
√
√
Lemma 1 holds for any γ∗ in the n-neighborhood of true γ. Hence, if we can obtain a n-consistent
estimator of γ, Lemma 1 can be used to construct a valid test of the null hypothesis H0 : β = β0
regardless of the instrumental relevance. The USSIV score test fails under the weak-instrument
asymptotics because even under the null hypothesis the estimator γb(β0 ) is not consistent for γ in
√
Cases 1 and 3. It should not, however, be possible to find a n- consistent estimator of γ when

instruments are weak for γ rendering it asymptotically unidentified. Hence it is not possible to find
a valid test for H0 : β = β0 (unless VW is uncorrelated with u and VX ) using tests like the USSIV

score test or for that matter, any “standard” test. However, as discussed earlier, under Cases 2 and
√
4 and against n-local alternatives, the USSIV score test can be used as a benchmark for the tests
based only on sub-sample one and the split-sample tests.

3.3

The Robins-Test:

In this section we propose a new test for H0 : β = β0 which is always valid and when the instruments
√
are strong it is asymptotically equivalent to the USSIV score test against n-local alternatives.
Our test is based on the general testing procedure proposed by Robins (2004), and we call this
the Robins-test. When the instruments are strong for γ, the Robins-test based on the split-sample
score function is quite generally less conservative than the projection test based on the split-sample
score statistic.
If γ belongs to the parameter space Θγ ⊆ Rmw , the projection test based on LM(β0 , γ0) rejects the
null hypothesis H0 : β = β0 at level α if
inf LM(β0 , γ0 ) > χ2m (1 − α)

γ0 ∈Θγ

(16)

The projection test can be very conservative when the instruments are strong for γ (and β).
Now we describe the Robins-test. Suppose that, given a specific value β0 , it is possible to construct
a 1 − ǫ confidence region for γ and let us denote it by Cγ (1 − ǫ, β0 ). The Robins-test rejects the null
hypothesis H0 : β = β0 if either Cγ (1 − ǫ, β0 ) is empty or if
inf
LMβ∗ (β0 , γ0 ) > χ2mx (1 − α).
γ0 ∈Cγ (1−ǫ,β0 )

Theorem 2 stated below shows that the size of the Robins-test cannot exceed α + ǫ and when the
instruments are strong for γ the test is asymptotically equivalent to the size-α USSIV score test
√
against n-local alternatives.

12

Before stating Theorem 2, we note that there exist different methods to construct confidence regions
like Cγ (1 −ǫ, β0 ). A general method of constructing the confidence region is as follows. Let T (γ0 |β0 )

denote a random function (test statistic) evaluated at β = β0 and γ = γ0 and suppose that it converges to ξ (which does not depend on any unknown parameters) when β0 and γ0 are the true values

of β and γ. Then a 1 − ǫ confidence region for γ is given by Cγ (1 − ǫ, β0 ) = {γ0 |T (γ0|β0 ) ≤ ξ(1 − ǫ)}
where ξ(1 − ǫ) is the (1 − ǫ)100-th quantile of the distribution of ξ. By definition Cγ (1 − ǫ, β0 )
contains the true value of γ with probability 1 − ǫ when the null hypothesis H0 : β = β0 is true.

If y2 is not missing, the above confidence region can be constructed based on the whole sample
otherwise we need to construct it based on sub-sample one. This method can be applied to the AR
statistic, the K statistic or the LR statistic which have pivotal asymptotic distributions. 6 When
the instruments are strong for γ, Cγ (1 − ǫ, β0 ), obtained by inverting
 the AR-test, the K-test or the

1

CLR-test, can only contain values γ0 such that kγ − γ0 k = O n− 2

[see Chaudhuri (2007)].

However, we consider a different choice of T (γ0 |β0 ) to construct Cγ (1 − ǫ, β0 ) in this paper. We
define
 


−1



′
′
′
b
b
 

▽γ J21 (β0 , γ0) ΠW 2 Z1 Z1 ΠW 2
▽γ J21 (β0 , γ0)

2
Cγ (1 − ǫ, β0 ) = γ0 T (γ0 |β0 ) = 1
≤ χmw (1 − ǫ) .


(y1 − X1 β0 − W1 γ0 )′ (y1 − X1 β0 − W1 γ0 )
 

n1
(17)

The only reason behind this choice is to maintain the uniformity of the presentation and we do
not make any optimality statement regarding the choice of Cγ (1 − ǫ, β0 ). However, given that the
√
(joint) tests for all structural coefficients (against n-local alternatives) using the K statistic or the
LR statistic (based on sub-sample one) and the split-sample score test have the same asymptotic
power under strong instruments, the choice of Cγ (1 − ǫ, β0 ) should not matter at least when the in-

struments are strong for γ. Lemma 2 summarizes the relevant asymptotic properties of Cγ (1−ǫ, β0 ).

Lemma 2: Let 1[δx =0] be a dummy variable taking value one if δx = 0 and zero if δx = − 21 where

δx is such that ΠX = O nδx . Similarly define the dummy variable 1[δw =0] where δw is such that

d
ΠW = O nδw . Suppose that β = β0 + √βn and γ = γ∗ + √dγn . When n → ∞,
T (γ0 |β0 ) =
d

−
→
6


−1
′
b ′ Z ′ Z1 Π
bW2
▽γ J21
(β0 , γ0 ) Π
▽γ J21 (β0 , γ0 )
W2 1
1
(y1
n1

− X1 β0 − W1 γ0 )′ (y1 − X1 β0 − W1 γ0 )

H′ PG H


σuu + (γ − γ0 )′ 1[δw =0] λ′W λW + σW W (γ − γ0 ) + 2σuW (γ − γ0 )

(18)

For the LR statistic, the asymptotic distribution conditional on an ancillary statistic is pivotal [see Moreira
(2003)].

13

p
where G =
λ
1 − ζ + (1 − 1[δw =0])ΨZW 2 and
W

i
h
p
√ p
H = lim ΨZu + ζ1[δx =0] λX dβ + [(1 − 1[δw =0] ) + 1[δw =0] n] ζλW + ΨZW 1 (γ − γ0 ) .
n→∞

We prove Lemma 2 in the Appendix. We list below those properties of Cγ (1 − ǫ, β0 ) which are
useful for the construction of the Robins-test.
d

1. It is clear from (18) that T (γ0 |β0 ) −
→ χ2mw when β0 and γ0 are the true values of β and γ.

Hence under the null hypothesis H0 : β = β0 , the confidence region Cγ (1 − ǫ, β0 ) contains the
true value of γ with probability 1 − ǫ.

2. When the instruments are strong for γ, the asymptotic distribution in (18)
 is1 that
 of a non−2
2
central χmw whose non-centrality parameter is finite iff kγ − γ0 k = O n
. Hence for
d

β0 = β − √βn , the confidence region Cγ (1 − ǫ, β0 ) can contain, with positive probability, only
√
those values that are in a n-neighborhood of the true γ.

Now we state the main result of this paper, Theorem 2.
Theorem 2: The Robins-test which rejects H0 : β = β0 if either Cγ (1 − ǫ, β0 ) given in (17) is
empty or if
inf
LMβ∗ (β0 , γ0 ) > χ2mx (1 − α) has the following properties. As n → ∞:
γ0 ∈Cγ (1−ǫ,β0 )

1. The size of the Robins-test never exceeds α + ǫ.
2. When γ is identified, the Robins-test is at least as powerful as the projection test described
√
in (16) and it is also asymptotically equivalent to the USSIV score test against n-local
alternatives.
We prove Theorem 2 in the Appendix. When the instruments are strong for γ, Theorem 2 states
that the Robins-test is asymptotically equivalent to the USSIV score test, our benchmark, against
√
n-local alternatives. However, unlike the USSIV score test, we have control over the size of
Robins-test when the instruments are weak for γ.
Theorem 2 also implies that the confidence region for β obtained by inverting the Robins-test (i.e.
the collection of all β0 ’s which cannot be rejected) has at least α + ǫ coverage probability asymptotically and when the instruments are strong for γ, this confidence region is asymptotically less
conservative than the projection confidence region based on the joint split-sample score test of β
and γ. Since, under weak instruments, any confidence region for β is unbounded with positive
probability making the expected length of the confidence region infinite, it is not possible to analytically compare the expected length of the confidence regions obtained from the projection test

14

and Robins-test.

7

Following Dufour and Taamouti (2005), we also show that (a possibly) infinite grid search is not
required to perform the Robins-test. The rejection rule for the Robins-test boils down to testing
quadratic inequalities in terms of γ [see Appendix]. Similarly we can construct the confidence region
for β based on the Robins-test by analytically solving quadratic inequalities in terms of β and γ.
However, we note that the existence of analytical methods for solving the inequalities exist because
the statistics LMβ∗ (β0 , γ0 ) and T (γ0 |β0 ) are ratios of quadratics in terms of β0 and γ0 and it may
not be possible to avoid grid search when the Robins-test is applied to other statistics.

4

Finite Sample Behavior of Split-Sample Score Tests

In this section we perform Monte-Carlo experiments to study the finite sample behavior of the
different score tests under different levels of instrument relevance and endogeneity. Our MonteCarlo design closely follows Zivot et al. (2006). We describe below the Monte-Carlo Design and the
data generating process for the model described in (1) – (3).

4.1

Monte-Carlo Design and Parameter Specifications:

The structural errors [u, VX , VW ] are generated by drawing n independent random samples from
N3 (0, Σ) where


 1 ρuX ρuW 


(19)
Σ=
0 

 ρXu 1


ρW u 0
1
If VX and VW are correlated, the level of endogeneity of the regressor X depends on the correlations
between VX and u, VX and VW and VW and u. Our choice of Σ in (19) simplifies the set-up by
ensuring that the level endogeneity of X depends only on the correlation between VX and u and
similarly the level of endogeneity of W depends only on the correlation between VW and u. Because
ρXW = 0, the overall endogeneity of the model can be measured by the quantity ρ2uX + ρ2uW . We
make three different choices for the pair (ρuX , ρuW ) – (0.5, 0.5), (0.1, 0.99) and (0.99, 0.1). X and
W are moderately (and equally) endogenous in the first case, X is highly endogenous and W is
mildly endogenous in the second case, X is mildly endogenous and W is highly endogenous in the
third case.

8

7

See Mikusheva (2006) for analytical comparison of the expected arc-length of the confidence regions (expressed
in spherical co-ordinates) for scalar β in a related context.
8
For positive-definiteness of Σ we need ρ2uX + ρ2uW < 1.

15

The instruments Z are generated by drawing n independent random samples from Nk (0, Q) independently of the structural errors. For simplicity we choose Q = Ik where k = 4 is chosen arbitrarily.
To our knowledge, there does not exist a universally accepted measure of instrumental relevance for
a particular structural coefficient in a linear IV model with more than one endogenous regressor.
However, for a model with a single endogenous regressor, the instruments are considered weak
for the structural coefficient if the concentration parameter is less than 10 [see Staiger and Stock
(1997)]. We follow Zivot et al. (2006) and impose the restriction(s) λ′X λW = 0 (and σXW = 0) such
that the concentration matrix given in (20) is diagonal where the first diagonal element corresponds
to the concentration parameter for β and the second one to the concentration parameter for γ.
Defining δx = − 12 for Cases 1 and 2 and δx = 0 for Cases 3 and 4 and similarly δw = − 12 for Cases
1 and 3 and δw = 0 for Cases 2 and 4, the concentration matrix is given by

µ=



 12 ′ 

1+2δx
2

λ′X



1+2δx
2

λ′X

′ 

 12





1  σXX σXW   n
 µβ 0 
 n
  σXX σXW 
  1+2δw


  1+2δw

 =
k
σW X σW W
0 µγ
n 2 λ′W
n 2 λ′W
σW X σW W

(20)

Π′X ΠW

We choose Π = [ΠX , ΠW ] such that
= 0 and such that µβ = 1 when the instruments are
weak for β and µβ = 10 when the instruments are strong for β. Similarly µγ = 1 when the instruments are weak for γ and
qµγ = 10 when the instruments are strong forqγ. In particular, the
µβ
ith element of ΠX is taken as
and the ith element of ΠW is taken as (−1)i µnγ for i = 1, . . . , k. 9
n
We choose the structural coefficients β = 1 and γ = 10. We take the sample size n = 100 and
randomly split the sample into two sub-samples where the first sub-sample contains n1 = [nζ] observations and the second sub-sample contains n2 = n − n1 observations. Finally, following the
missing y2 motivation, we assume that y2 is not observable (delete y2 from sub-sample two). The
results reported below are based on 10,000 Monte-Carlo trials. The instrument matrix Z is kept
fixed over the 10,000 trials.
The null hypothesis of interest is H0 : β = β0 and we compare the finite sample behaviors of the
USSIV score test, the Robins-test and the projection test based on the split-sample score statistic
under the null and alternative hypotheses. 10 We also compare the finite sample behaviors of these
tests with two other tests considered by Zivot et al. (2006) – the partial K-test and the projection
test based on the AR statistic [see Appendix]. The partial K-test and the projection test based on
the AR statistic are performed based on sub-sample one alone, so that power of all the tests (except
9
10

In Section 4.3, we also consider strong instrument for β (γ) characterized by µβ = 100 (µγ = 100).
For the Robins-test we always choose ǫ = α.

16

the AR-test) are asymptotically equal when the instruments are strong for both β and γ.

4.2

Rejection Rates when the Null Hypothesis is True:

Tables 1 and 2 summarize the nominal size of the above five tests for different levels of endogeneity,
instrument relevance, critical values and proportion of observations in sub-sample one. As discussed
before, none of the tests are over-sized when the instruments are strong for γ (i.e. under Cases 2
and 4). Under Cases 1 and 3, the USSIV score test over-rejects the null hypothesis when it is true.
The rate of over-rejection of the USSIV score test also increases with the level of endogeneity of W .
This should not be surprising because the asymptotic-bias of the constrained USSIV estimator of γ
increases with ρuW (the asymptotic bias does not depend on the level of endogeneity of X and under
the null hypothesis it does not depend on the correlation between VX and VW ). The projection
tests based on the AR and the split-sample score statistics are conservative. Theorem 2 gives an
upper bound of α + ǫ for the level of the Robins-test when the instruments are weak for γ. However,
simulation results indicate that the upper bound is overly conservative and the nominal size of the
Robins-test does not exceed α. Unlike the USSIV score test, the partial K-test is not over-sized even
when the instruments are weak for γ. Similar to the findings of Guggenberger and Smith (2005)
and Zivot et al. (2006), our simulation results show that the partial K-test is, in fact, size-distorted
downward when the instruments are weak for γ. [Chaudhuri (2007) shows how the downward sizedistortion of the partial K-test affects its power.]
[INSERT TABLE-1 AND TABLE-2 HERE.]

4.3

Rejection Rates when the Null Hypothesis is False:

Figures 1 - 3 plot the (nominal) power curves of the different tests using the 5% critical values
under different specifications of the error covariance. In Figure 1, both X and W are moderately
endogenous, in Figure 2, X is mildly endogenous whereas W is highly endogenous and in Figure
3, X is highly endogenous whereas W is mildly endogenous. We choose ζ = 75% without loss of
generality. Other (non-extreme and reasonable) choices of ζ do not change the results (Figures 7 12 use ζ = 25% and Figures 13 - 18 use ζ = 50%). To highlight the fact that when the instruments
are weak for γ, the high power of the USSIV score test comes at the cost of its upward size-distortion
and the validity of the Robins-test and projection test based on split-sample score statistic comes
at the cost ot their low power, we choose not to plot the size-adjusted powers.
[INSERT FIGURES 1 – 3 HERE.]
The projection test based on the split-sample score statistic and the Robins-test are extremely
17

conservative when the instruments are weak for γ. The USSIV score test has high power in all
cases, but because of its upward size-distortion under Cases 1 and 3, it cannot be reliably used
in practice. The partial K-test and the projection test based on the AR statistic are not oversized and at the same time are more powerful than the other tests. However, the power of neither
of these two tests dominate each other uniformly. Theorem 2 shows that when instruments are
strong for γ, the Robins-test is asymptotically equivalent to the USSIV score test and hence more
powerful than the projection test based on the split-sample score statistic. This result cannot be
verified by simulations when “strong-instruments” for γ is characterized by µγ = 10. However,
when we consider strong instruments by taking the corresponding concentration parameter to be
100, the claims of Theorem 2 are verified. We note that the characterization of strong instrument,
by taking the concentration parameter at least 10, is proposed by Staiger and Stock (1997) and
Stock and Yogo (2005) to ensure that the relative bias of the 2SLS estimator with respect to that of
the OLS estimator (of the corresponding structural parameter) does not exceed 10%. This characterization need not be appropriate under our framework. In Figures 4 - 6, we plot the same power
curves taking the concentration parameter to be 100 when the corresponding instruments are strong.
[INSERT FIGURES 4 – 6 HERE.]
In Figures 4 – 6, the power of the Robins-test dominates the power of the projection test based on
the split-sample score statistic when the instruments are strong for γ. When the instruments are
weak for γ, powers of both these tests are close to zero and neither test can distinguish the true
value of β from the false ones. However, these tests are valid unlike the USSIV score test which is
over-sized when the instruments are weak for γ. We also note that when the instruments are strong
for both β and γ, the powers of the Robins-test and the projection test based on the split-sample
score statistic are very close to that of the partial-K test and the USSIV score test, and are greater
than that of the projection test based on the AR-statistic.
The simulations as a whole encourage the use of the partial K-test and the projection test based
on the AR statistic. This is similar to the conclusions of Zivot et al. (2006). We also note that the
instruments for γ have to be very strong for the asymptotic equivalence between the USSIV score
test and the Robins-test to hold. However, the Robins-test and the projection test based on the
split-sample score statistic could possibly be used reliably without over-rejecting the true parameter
value of β under all the cases discussed in this paper.

18

5

Conclusion

In the present paper we show how to construct valid tests for subsets of structural coefficients by
splitting the sample in two parts or, in other words, by combining information from two “un-related”
samples one of which need not contain information on the dependent variable. The USSIV score
√
test for subsets of structural coefficients (H0 : β = β0 ) against n-local alternatives is as powerful
as the partial K-test (based on sub-sample one) when the instruments are strong for the remaining
structural coefficients (γ), but it is severely over-sized otherwise. On the other hand, the Robinstest is never over-sized and at the same time it is asymptotically as powerful as the USSIV score
test when the instruments are strong for the remaining structural coefficients. However, moderate
strength of instruments (for example, when the corresponding concentration parameter takes the
value 10) may not be enough to ensure the asymptotic equivalence between USSIV score test and
the Robins-test. In any case, the Robins-test can be reliably used for testing subsets of structural
coefficients in a linear Instrumental Variables model. The projection test based on the (joint) splitsample score statistic is also never over-sized, but can be extremely conservative. The power of
Robins-test is more than that of the projection test based on the (joint) split-sample score statistic
when the instruments are strong for the remianing structural coefficients. Similar to the finding of
Zivot et al. (2006), our simulation results also indicate that, for the sample sizes considered here,
the projection test based on the AR statistic and the partial K-test (both based on sub-sample one)
are never over-sized and not less powerful than the split-sample tests described in the present paper.
In this paper, we introduced Robins’ method for testing hypotheses on subsets of parameters and
subsequently constructing valid confidence regions under partial identification. It is a projectionbased method that can be substantially less conservative than projections from pivotal statistics
for testing the significance of all parameters jointly. The application of Robins’ method requires
two statistics: a valid test of the parameters of interest when the nuisance parameters are known;
a valid confidence set for the nuisance parameters when the parameters of interest are known. In
principle, Robin’s method can be applied to the linear IV model in a non-split sample context using,
for example, the results of Kleibergen (2004), and to nonlinear models estimated by the generalized
method of moments using the results of Kleibergen (2005). These extensions are the subject of our
future research.

19

6

Appendix

6.1

Proofs of Results:

It helps to prove Lemma 1 before proving Theorem 1.
Proof of Lemma 1: Using Assumption A, it is easy to see that
1
1
1
1
d
d
bW2 −
b X2 −
→ CW +(1−ζ)− 2 (1−1[δw =0] )Q− 2 ΨZW 2.
n−δx Π
→ CX +(1−ζ)− 2 (1−1[δx =0] )Q− 2 ΨZX2 and n−δw Π

Then it follows directly from Assumption A that


b′
b
1 Π
X2
Z ′ M b W 2 Z1 ΠnX2
δx
n1 nδx 1 Z1 Π

q

nδw

1
n1

− 21

b′
Π
1 γ∗
X2
Z ′ M b W 2 y1 −X1√βn0 −W
nδx 1 Z1 Π
1
nδw

(y1 − X1 β0 − W1 γ∗ )′ (y1 − X1 β0 − W1 γ∗ )
d

and hence LMβ∗ (β0 , γ∗) −
→

1′

(B′ B)− 2 B′ D
−
→
√
σuu
d

D′ P B D
. [Q.E.D.]
σuu

Proof of Theorem 1: We restrict attention to Cases 2 and 4. Using (10) and the proof of Lemma
1, we note that

h
i

−1
d
d
′
′
′
′
b
b
1. y1 − X1 β0 − W1 b
γ (β0 ) = In1 − W1 ΠW 2 Z1 W1
ΠW 2 Z1 u1 + Z1 ΠX √βn + VX1 √βn
P
bW2 −
2. Π
→ CW .

Hence, Assumption A gives that
h

i 1 b′
b′
b X2 − 2 Π
Π
1 Π
′
X2
X2
Z
M
Z
b W 2 1 nδx
n1 nδx 1 Z1 Π
nδx
q

1
n1



Ik −

1
n1

Z1′ W1
n1

P

(y1 − X1 β0 − W1 b
γ (β0 ))′ (y1 − X1 β0 − W1 γb(β0 )) −
→ σuu and



b ′ Z1′ W1
Π
W 2 n1

−1

b′
Π
W2



Z′
√1
n1

h
i
d
d
u1 + Z1 ΠX √βn + VX1 √βn

(y1 − X1 β0 − W1 b
γ (β0 ))′ (y1 − X1 β0 − W1 b
γ (β0 ))

1′

(E′ E)− 2 E′ F
−
→
√
σuu
d


 p


√
where E = MλW √1−ζ λX 1 − ζ + (1 − 1δx =0 )ΨZX2 and F = ΨZu1 + ζ1[δx =0] λX dβ . Under
Cases 2 and 4 it is easy to see that by definition B = E and D = F and hence
LMβ (β0 ) = LMβ∗ (β0 , γ∗) + op (1)
Direct application of Lemma 1 proves Theorem 1. [Q.E.D.]

20

(21)

Proof of Lemma 2: Using Lemma 1, we get
 b′

bW2
ΠW 2 Z1′ Z1 Π
nδw n1 nδw

− 12

b′
Π
Z1′
W2 √
nδw
n1

q

1
n1

h
i
d
d
u1 + Z1 ΠX √βn + VX1 √βn + Z1 ΠW (γ − γ0 ) + VW 1 (γ − γ0 )

(y1 − X1 β0 − W1 γ0 )′ (y1 − X1 β0 − W1 γ0 )
1′

(G′ G)− 2 G′ H
−
→ q


σuu + (γ − γ0 )′ 1[δw =0] λ′W λW + σW W (γ − γ0 ) + 2σuW (γ − γ0 )
d

d

Hence T (γ0 |β0 ) −
→

H′ PG H


. [Q.E.D]
σuu + (γ − γ0 )′ 1[δw =0] λ′W λW + σW W (γ − γ0 ) + 2σuW (γ − γ0 )

Proof of Theorem 2:
Part 1: Using Lemma 2, it is straightforward to see





∗
2
inf
LMβ (β0 , γ0) > χmx (1 − α)
P rβ0 {Cγ (1 − ǫ, β0 ) = ∅} ∪ {Cγ (1 − ǫ, β0 ) 6= ∅} ∩
γ0 ∈Cγ (1−ǫ,β0 )



∗
2
/ Cγ (1 − ǫ, β0 )} ∪
inf
LMβ (β0 , γ0 ) > χmx (1 − α)
≤ P rβ0 {γ ∈
γ0 ∈Cγ (1−ǫ,β0 )6=∅
	


≤ P rβ0 {γ ∈
/ Cγ (1 − ǫ, β0 )} ∪ LMβ∗ (β0 , γ) > χ2mx (1 − α)


≤ P rβ0 [γ ∈
/ Cγ (1 − ǫ, β0 )] + P rβ0 LMβ∗ (β0 , γ) > χ2mx (1 − α)
= ǫ+α

Part 2: When γ is asymptotically identified, Lemma 2 gives that only values from the

√

n-

neighborhood of the true γ can be contained in Cγ (1 − ǫ, β0 ) with positive probability. Hence
d
inf
LMβ∗ (β0 , γ0) is attained
for any β0 such that β − β0 = √βn , the value γbinf (β0 ) where the
γ0 ∈Cγ (1−ǫ,β0 )
√
should be in the n-neighborhood of the true γ. (21) gives
inf

γ0 ∈Cγ (1−ǫ,β0 )

LMβ∗ (β0 , γ0 ) = LMβ∗ (β0 , b
γinf (β0 )) = LMβ (β0 ) + op (1).

(22)

The following steps show that the Robins-test is more asymptotically powerful than the projection
test when γ is identified.


− α)


≥ P rβ LM(β0 , b
γ (β0 )) ≤ χ2m (1 − α)


= P rβ LMβ (β0 ) ≤ χ2m (1 − α)


≥ P rβ LMβ (β0 ) ≤ χ2mx (1 − α) .
P rβ

inf LM(β0 , γ0 ) ≤

χ2m (1

γ0 ∈Θγ

21



Hence the USSIV score test is more powerful that projection test. Using (22) we can see that the
Robins-test is asymptotically more powerful than projection test whenever γ is identified. [Q.E.D.]

6.2

K-Test, Partial K-Test and CLR-Test:

I) K-Test [see Kleibergen (2002)] rejects the null hypothesis H : β = β0 , γ = γ0 at level α if
K(β0 , γ0 ) =

(y1 − X1 β0 − W1 γ0 )′ PZ1 Π(β
˜ 0 ,γ0 ) (y1 − X1 β0 − W1 γ0 )
1
(y
n1 1

− X1 β0 − W1 γ0 )′ PZ1 (y1 − X1 β0 − W1 γ0 )

> χ2m (1 − α)

i
h
i
h
˜ 0 , γ0 ) = Π
˜X, Π
˜W
˜ X (β0 , γ0) = (Z ′ Z1 )−1 Z ′ X1 − (y1 − X1 β0 − W1 γ0 ) σXu (β0 ,γ0 ) ,
where Π(β
where Π
1
1
σuu (β0 ,γ0 )
h (β0 ,γ0 )
i
˜ W (β0 , γ0 ) = (Z ′ Z1 )−1 Z ′ W1 − (y1 − X1 β0 − W1 γ0 ) σW u (β0 ,γ0 ) , σU (β0 , γ0 ) = 1 (y1 −X1 β0 −W1 γ0 )′ MZ1 (y1 −
Π
1
1
σuu (β0 ,γ0 )
n1

X1 β0 − W1 γ0 ), σXu (β0 , γ0 ) =
W1 γ0 ).

1
X ′ M (y − X1 β0 − W1 γ0 )
n1 1 Z1 1

and σW u (β0 , γ0) =

1
W1′ MZ1 (y1 − X1 β0 −
n1

II) Partial K-Test [see Kleibergen (2004)] rejects the null hypothesis H0 : β = β0 at level α if
K(β0 , γ˜ (β0 )) =

(y1 − X1 β0 − W1 γ˜ (β0 ))′ PZ1 Π(β
˜ (β0 ))
˜ 0 ,˜
γ (β0 )) (y1 − X1 β0 − W1 γ
1
(y1
n1

− X1 β0 − W1 γ˜ (β0 ))′ MZ1 (y1 − X1 β0 − W1 γ˜ (β0 ))

> χ2mx (1 − α)

˜ ′ (β0 , γ˜ (β0 ))Z ′ (y1 − X1 β0 − W1 γ˜ (β0 )) = 0.
where γ˜ (β0 ) solve Π
1
W
III) CLR-Test [see Moreira (2003)] rejects the null hypothesis H : β = β0 , γ = γ0 at level α if


q
1 ′
2
′
′
′
′
′
′
2
S S − T T + [S S + T T ] − 4 [S ST T − (S T ) ] > ξLR (1 − α, T ′T = τ, k)
L(β0 , γ0 ) =
2
where S = p

b=
Ω

1
n1

1

1
1
b −1 A(A′ Ω
b −1 A)− 21 ,
(Z1′ Z1 )− 2 Z1′ (y1 −X1 β0 −W1 γ0 ), T = (Z1′ Z1 )− 2 Z1′ [y1 , X1 , W1 ] Ω

σuu (β0 , γ0 )
[y1 , X1 , W1 ]′ MZ1 [y1 , X1 , W1 ] and A = [(β0′ , γ0′ )′ , Im ]. Finally, ξ(1 − α, τ, k) is the (1 − α)100-

th quantile of the null-distribution of LR0 given T ′ T = τ and the number of instruments k.

6.3

Rejection Rules for Projection Type Tests:

We discuss the rejection rules for different tests testing the null hypothesis H0 : β = β0 . Following
our Monte-Carlo setting, we do it for the special case where γ is scalar. However, it is not hard to
22

extend the results to vector valued γ. Our discussion is based on Dufour and Taamouti (2005).
I) Projection Test based on AR(β0 , γ) based on sub-sample one rejects H0 : β = β0 at level α if
inf AR(β0 , γ) = inf

γ∈Θγ

γ∈Θγ

(y1 − X1 β0 − W1 γ)′ PZ1 (y1 − X1 β0 − W1 γ)
> χ2k (1 − α)
1
′ M (y − X β − W γ)
(y
−
X
β
−
W
γ)
1
1
0
1
Z
1
1
0
1
1
n1

i.e. if there does not exist any value of γ such that a0 γ 2 − 2b0 γ + c0 ≤ 0 where a0 = W1′ A0 W1 ,
1
b0 = W1′ A0 (y1 −X1 β0 ), c0 = (y1 −X1 β0 )′1 A0 (y1 −X1 β0 ) and A0 = PZ1 − χ2k (1−α)MZ1 . Equivalently,
n1
reject H0 : β = β0 at level α if
 2
	
b0 − a0 c0 < 0, a0 > 0 ∪ {a0 = b0 = 0, c0 > 0}
II) Projection Test based on LM(β0 , γ) rejects H0 : β = β0 at level α if

inf LM(β0 , γ) = inf

γ∈Θγ


−1
′
b ′2 Z1′ Z1 Π
b2
(β0 , γ) Π
▽J21
▽J21 (β0 , γ)

γ∈Θγ 1 (y1
n1

− X1 β0 − W1

γ)′ (y

1

− X1 β0 − W1 γ)

> χ2m (1 − α)

i.e. if there does not exist any value of γ such that a1 γ 2 − 2b1 γ + c1 ≤ 0 where a1 = W1′ A1 W1 ,
1 2
χ (1 − α)In1 .
b1 = W1′ A1 (y1 − X1 β0 ), c1 = (y1 − X1 β0 )′1 A1 (y1 − X1 β0 ) and A1 = PZ1 Πb 2 −
n1 m
Equivalently, reject H0 : β = β0 at level α if
 2
	
b1 − a1 c1 < 0, a1 > 0 ∪ {a1 = b1 = 0, c1 > 0}
III) Robins-Test rejects H0 : β = β0 at level atmost α(+ǫ) if
{Cγ (1 − ǫ, β0 ) = ∅} ∪
i.e. iff



inf

γ∈Cγ (1−ǫ,β0 )

LMβ∗ (β0 , γ)

{Cγ (1 − ǫ, β0 ) ∩ Dγ (1 − α, β0 )} = ∅

>

χ2m (1

− α)




	

	
where Cγ (1 − ǫ, β0 ) = γ0 |a2 γ02 − 2b2 γ0 + c2 ≤ 0 and Dγ (1 − α, β0 ) = γ0 |a3 γ02 − 2b3 γ0 + c3 ≤ 0

23

and
a2 = W1′ A2 W1 ,

a3 = W1′ A3 W1

b2 = W1′ A2 (y1 − X1 β0 ),

b3 = W1′ A3 (y1 − X1 β0 )

c2 = (y1 − X1 β0 )′1 A2 (y1 − X1 β0 ),
1
A2 = PZ1 Πb W 2 − χ2mw (1 − ǫ)In1 ,
n1

c3 = (y1 − X1 β0 )′1 A3 (y1 − X1 β0 )
1 2
i −
χ (1 − α)In1
A3 = PhM
b
Z1 ΠX2
b
Z1 Π
n1 mx
W2

Defining ∆i = b2i − ai ci for i = 2, 3, Robins-test rejects H0 : β = β0 at level at most α + ǫ if any one
of the following six mutually exclusive conditions are satisfied:
1. {∆i < 0, ai > 0} ∪ {ai = bi = 0, ci > 0} for i = 2 and/or i = 3, i.e. if at least one of the
intervals Cγ (1 − ǫ, β0 ) and Dγ (1 − α, β0 ) is empty.


cj
ci
2. ai = 0, bi > 0, aj = 0, bj < 0,
, for i, j = 2, 3 and i 6= j i.e. if the intervals are of
<
2b
2b
j
i




cj
cj
ci
ci
where
, +∞ and −∞,
<
.
the form
2bi
2bj
2bj
2bi
(
)
p
bj + ∆j
ci
3. ai = 0, bi > 0, aj > 0, ∆j ≥ 0,
<
for i, j = 2, 3 and i 6= j i.e. if the intervals
aj
2bi
"
p
p
p #


bj + ∆j
bj − ∆j bj + ∆j
ci
ci
where
, +∞ and
,
<
.
are of the form
2bi
aj
aj
aj
2bi
(
)
p
bj − ∆j
ci
4. ai = 0, bi < 0, aj > 0, ∆j ≥ 0,
for i, j = 2, 3 and i 6= j i.e. if the intervals
>
aj
2bi
"
p
p
p #


bj − ∆j bj + ∆j
bj − ∆j
ci
ci
and
where
,
>
.
are of the form −∞,
2bi
aj
aj
aj
2bi
(
p
p )
√
√
bj + ∆j bi + ∆i
bj − ∆j
bi − ∆i
>
,
<
for i, j = 2, 3
5. ai > 0, ∆i ≥ 0, aj < 0, ∆j ≥ 0,
ai
aj
ai
aj
"
p #
√
√ 

bj + ∆j
bi − ∆i bi + ∆i
and i 6= j i.e. if the intervals are of the form
,
and −∞,
∪
ai
ai
aj
#
"
p
p
p
√
√
bj + ∆j
bj − ∆j
bj − ∆j
bi − ∆i
bi + ∆i
, +∞ where
>
and
<
.
aj
ai
aj
ai
aj
(
p )
√
bj + ∆j
bi − ∆i
6. ai > 0, ∆i ≥ 0, aj > 0, ∆j ≥ 0,
for i, j = 2, 3 and i 6= j i.e. if the in>
ai
aj
"
p
p #
√
√ 
√

bj − ∆j bj + ∆j
bi − ∆i
bi − ∆i bi + ∆i
and
where
,
,
>
tervals are of the form
ai
ai
aj
aj
ai
p
bj + ∆j
.
aj
24

7

Tables and Figures
Error Correlations
ACV: α (in %)

USSIV-Test
Proj SS-Test
ζ = 25% Robins-Test a
Proj AR-Test
Partial K-Test
USSIV-Test
Proj SS-Test
ζ = 50%
Robins-Test
Proj AR-Test
Partial K-Test
USSIV-Test
Proj SS-Test
ζ = 75%
Robins-Test
Proj AR-Test
Partial K-Test
USSIV-Test
Proj SS-Test
ζ = 25%
Robins-Test
Proj AR-Test
Partial K-Test
USSIV-Test
Proj SS-Test
ζ = 50%
Robins-Test
Proj AR-Test
Partial K-Test
USSIV-Test
Proj SS-Test
ζ = 75%
Robins-Test
Proj AR-Test
Partial K-Test

ρuX = 0.5, ρuW = 0.5 ρuX = 0.1, ρuW = 0.99
1
5
10
1
5
10
Case I: µβ = 1 and µγ = 1
1.2 6
11.8
3.3 10.8
17.3
0 0.1
0.3
0.1 0.8
2.5
0 0.1
0.2
0
0.6
2.7
0.1 1.1
2.5
2.5
7
11.4
0.1 1.7
5.7
1.2 6.2
11.7
1.5 7.2
13.2
6.7 14.2
20.5
0 0.1
0.5
0.2 1.2
2.9
0
0
0.4
0
1.1
3.2
0.1 0.6
1.7
1.2 4.2
7.7
0.1 1.8
5
1
5.5
10.8
2.7 9
15.7
10.1 18.1
24.9
0 0.2
0.6
0.1 0.9
2.6
0 0.1
0.5
0
0.1
2.9
0.1 0.7
1.9
0.7 3.2
6.5
0.2 2.3
6
1
5.5
11.1
Case III: µβ = 10 and µγ = 1
1.2 5.8
11.8
2.3
9
15.4
0 0.1
0.4
0.1 0.8
2.4
0 0.1
0.3
0
0.5
2.4
0.1 1.1
2.5
2.5
7
11.4
0.1 1.4
4.3
1
5.3
10.6
1.5 6.4
12.6
3.9 10.3
16.7
0 0.1
0.5
0.1 1.1
2.5
0 0.1
0.4
0
1
3
0.1 0.6
1.7
1.2 4.2
7.7
0.1 1.7
4.9
0.9
5
10.4
1.8 7.5
13.6
5.8
13
19.8
0 0.2
0.6
0.2 1.1
2.7
0 0.1
0.5
0.1 0.9
2.9
0.1 0.7
1.9
0.7 3.2
6.5
0.2 2.1
5.2
0.8
5
10.1

ρuX = 0.99, ρuW = 0.1
1
5
10
1.1
0
0
0.1
0.1
1.6
0
0
0.1
0.1
2.2
0
0
0
0.2

6.1
0.1
0
1
1.2
6.9
0.1
0
0.5
1.3
8.4
0.1
0.1
0.5
1.7

11.9
0.3
0.2
2.1
4
12.9
0.4
0.3
1.2
4.5
14.7
0.5
0.5
1.5
4.7

1
0
0
0.1
0.1
1.4
0
0
0.1
0.1
1.7
0
0
0
0.2

5.9
0.1
0
1
1.3
6.4
0.1
0
0.5
1.3
7.2
0.1
0.1
0.5
1.7

12.3
0.3
0.2
2.1
4.1
12.4
0.4
0.3
1.2
4.2
13.2
0.4
0.3
1.5
4.6

Table 1: Empirical Size of different tests for H0 : β = β0 are computed based on 10,000 Monte-Carlo
trials. USSIV Score test, Projection from Split-sample Score test (SS) and Robins test combine
observations from both sub-samples. Projection from AR test and partial-K tests are based on
sub-sample one containing n1 = [nζ] observations. [k = 4, ρXW = 0, n = 100]
By Theorem 2, the level of the Robins-test is always lesser than α + ǫ (= 2ACV by our specification), but is
asymptotically equal to α if the instruments are strong for γ.
a

25

Error Correlations
ACV: α (in %)
USSIV-Test
Proj SS-Test
ζ = 25% Robins-Test a
Proj AR-Test
Partial K-Test
USSIV-Test
Proj SS-Test
ζ = 50%
Robins-Test
Proj AR-Test
Partial K-Test
USSIV-Test
Proj SS-Test
ζ = 75%
Robins-Test
Proj AR-Test
Partial K-Test
USSIV-Test
Proj SS-Test
ζ = 25%
Robins-Test
Proj AR-Test
Partial K-Test
USSIV-Test
Proj SS-Test
ζ = 50%
Robins-Test
Proj AR-Test
Partial K-Test
USSIV-Test
Proj SS-Test
ζ = 75%
Robins-Test
Proj AR-Test
Partial K-Test

ρuX = 0.5, ρuW = 0.5 ρuX = 0.1, ρuW = 0.99
1
5
10
1
5
10
Case II: µβ = 1 and µγ = 10
1.1 5.9
11.6
1 5.9
11.8
0.1 0.7
2.1
0.1 1.3
3.4
0 0.3
1.9
0.1 1.8
5.7
1.3 4.7
8.4
2.7 7.5
12.2
0.7 4.8
9.6
1.2 6.2
11.7
1.1 5.9
11.3
1.2 6.1
11.7
0.1 1.1
2.9
0.2 1.4
3.1
0.1 1.1
3.9
0.2 2.3
5.9
0.9 3.6
6.7
1.2 4.3
7.9
0.8 5
10.2
1 5.6
11
1.7 6.9
12.8
1.4 6.7
12.8
0.1 1.1
2.7
0.1 0.9
2.9
0.1 1.2
3.9
0.1 1.4
4.5
0.7 3.1
5.9
0.8 3.3
6.7
0.9 4.9
10.6
0.9 5.4
11
Case IV: µβ = 10 and µγ = 10
0.9 6.1
11.6
1 5.9
11.7
0 0.6
2.1
0.1 1.3
3.5
0 0.3
2.1
0 1.7
5.6
1.3 4.7
8.4
2.7 7.5
12.2
0.5 3.9
8.6
0.9 5.4
10.7
1.2 5.7
11
1.2 5.7
11.5
0.2 1.2
2.8
0.2 1.5
3.1
0 1.1
3.9
0.2 2.2
5.8
0.9 3.6
6.7
1.2 4.3
7.9
0.8 4.6
9.8
1 5.2
10.3
1.3 6.4
12.6
1.4 6.4
12.4
0.2 1
2.8
0.2 1.2
3.1
0.1 1.2
4.1
0.2 1.8
5.2
0.7 3.1
5.9
0.8 3.3
6.7
0.8 4.8
9.8
1 5.1
10.3

ρuX = 0.99, ρuW = 0.1
1
5
10
1
0
0
1.1
0.3
1.1
0.1
0
0.8
0.7
1.7
0.2
0.1
0.6
0.8

5.7
0.5
0.2
4.1
3.3
5.6
0.9
0.8
3.1
4.2
6.7
1.1
1
2.9
4.7

11.4
2
1.6
7.2
7.7
11.1
2.5
3.1
6.6
9.3
12.5
2.6
3.3
5.8
9.7

0.8
0
0
1.1
0.3
1.3
0.1
0
0.8
0.7
1.5
0.2
0.1
0.6
0.8

5.8
0.4
0.2
4.1
3.2
5.8
1.1
0.9
3.1
4.1
6.4
1
1
2.9
4.7

12
1.7
1.6
7.2
7.6
11.2
2.6
3.4
6.6
9.3
12.3
2.7
3.7
5.8
9.6

Table 2: Empirical Size of different tests for H0 : β = β0 are computed based on 10,000 Monte-Carlo
trials. USSIV Score test, Projection from Split-sample Score test (SS) and Robins test combine
observations from both sub-samples. Projection from AR test and partial-K tests are based on
sub-sample one containing n1 = [nζ] observations. [k = 4, ρXW = 0, n = 100]
By Theorem 2, the level of the Robins-test is always lesser than α + ǫ (= 2ACV by our specification), but is
asymptotically equal to α if the instruments are strong for γ.
a

26

Power Curves when k = 4, n = 100, ρuX = 0.5, ρuW = 0.5, ρXW = 0 and ζ = 0.75
Case 1: µβ = 1, µγ = 1

Case 2: µβ = 1, µγ = 10

0.2
0.25
0.15

0.2
0.15

0.1

0.1
0.05
0.05
0
−5

0
β−β0

0
−5

5

Case 3: µβ = 10, µγ = 1

0
β−β0

5

Case 4: µβ = 10, µγ = 10

0.5

0.8
0.7

0.4
0.6
0.5

0.3

0.4
0.2

0.3
0.2

0.1

0.1
0
−0.5

0
β−β0
USSIV

0
−0.5

0.5

Proj−SS

Robins

0
β−β0
Proj−AR

Partial−K

0.5

5%

Figure 1: Rejection Rates for tests of H0 : β = β0 are computed based on 10,000 Monte-Carlo
Trials. Weak instrument characterized by µ = 1 and strong instrument by µ = 10.

27

Power Curves when k = 4, n = 100, ρuX = 0.1, ρuW = 0.99, ρXW = 0 and ζ = 0.75
Case 1: µβ = 1, µγ = 1

Case 2: µβ = 1, µγ = 10
0.25

0.2

0.2
0.15
0.15
0.1
0.1
0.05

0.05

0
−5

0
β−β0

0
−5

5

Case 3: µβ = 10, µγ = 1

0
β−β0

5

Case 4: µβ = 10, µγ = 10

0.5
0.6
0.4

0.5

0.3

0.4
0.3

0.2
0.2
0.1

0.1

0
−0.5

0
β−β0
USSIV

0
−0.5

0.5
Proj−SS

Robins

0
β−β0
Proj−AR

Partial−K

0.5
5%

Figure 2: Rejection Rates for tests of H0 : β = β0 are computed based on 10,000 Monte-Carlo
Trials. Weak instrument characterized by µ = 1 and strong instrument by µ = 10.

28

Power Curves when k = 4, n = 100, ρuX = 0.99, ρuW = 0.1, ρXW = 0 and ζ = 0.75
Case 1: µβ = 1, µγ = 1

Case 2: µβ = 1, µγ = 10

0.4

0.8

0.3

0.6

0.2

0.4

0.1

0.2

0
−5

0
β−β0

0
−5

5

Case 3: µβ = 10, µγ = 1

0
β−β0

5

Case 4: µβ = 10, µγ = 10

0.5

0.8

0.4
0.6
0.3
0.4
0.2
0.2

0.1
0
−0.5

0
β−β0
USSIV

0
−0.5

0.5

Proj−SS

Robins

0
β−β0
Proj−AR

Partial−K

0.5

5%

Figure 3: Rejection Rates for tests of H0 : β = β0 are computed based on 10,000 Monte-Carlo
Trials. Weak instrument characterized by µ = 1 and strong instrument by µ = 10.

29

Power Curves when k = 4, n = 100, ρuX = 0.5, ρuW = 0.5, ρXW = 0 and ζ = 0.75
Case 1: µβ = 1, µγ = 1

Case 2: µβ = 1, µγ = 100
0.3

0.2

0.25
0.15

0.2
0.15

0.1

0.1
0.05
0.05
0
−5

0
β−β0

0
−5

5

0
β−β0

Case 3: µβ = 100, µγ = 1

5

Case 4: µβ = 100, µγ = 100

0.6
0.8

0.5

0.6

0.4
0.3

0.4

0.2
0.2

0.1
0
−0.5

0
β−β0
USSIV

0.5

Proj−SS

Robins

0
−0.5

Proj−AR

0
β−β0
Partial−K

0.5

5%

Figure 4: Rejection Rates for tests of H0 : β = β0 are computed based on 10,000 Monte-Carlo
Trials. Weak instrument characterized by µ = 1 and strong instrument by µ = 100.

30

Power Curves when k = 4, n = 100, ρuX = 0.1, ρuW = 0.99, ρXW = 0 and ζ = 0.75
Case 1: µβ = 1, µγ = 1

Case 2: µβ = 1, µγ = 100

0.2
0.2
0.15
0.15
0.1
0.1
0.05

0.05

0
−5

0
β−β0

0
−5

5

Case 3: µβ = 100, µγ = 1

0
β−β0

5

Case 4: µβ = 100, µγ = 100

0.6
0.8

0.5
0.4

0.6

0.3

0.4

0.2
0.2

0.1
0
−0.5

0
β−β0
USSIV

0.5
Proj−SS

Robins

0
−0.5

0
β−β0
Proj−AR

Partial−K

0.5
5%

Figure 5: Rejection Rates for tests of H0 : β = β0 are computed based on 10,000 Monte-Carlo
Trials. Weak instrument characterized by µ = 1 and strong instrument by µ = 100.

31

Power Curves when k = 4, n = 100, ρuX = 0.99, ρuW = 0.1, ρXW = 0 and ζ = 0.75
Case 1: µβ = 1, µγ = 1

Case 2: µβ = 1, µγ = 100

0.4

0.8

0.3

0.6

0.2

0.4

0.1

0.2

0
−5

0
β−β0

0
−5

5

Case 3: µβ = 100, µγ = 1

0
β−β0

5

Case 4: µβ = 100, µγ = 100

0.6
0.8
0.5
0.6

0.4
0.3

0.4

0.2
0.2

0.1
0
−0.5

0
β−β0
USSIV

0.5
Proj−SS

Robins

0
−0.5

0
β−β0
Proj−AR

Partial−K

0.5
5%

Figure 6: Rejection Rates for tests of H0 : β = β0 are computed based on 10,000 Monte-Carlo
Trials. Weak instrument characterized by µ = 1 and strong instrument by µ = 100.

32

Power Curves when k = 4, n = 100, ρuX = 0.5, ρuW = 0.5, ρXW = 0 and ζ = 0.25
Case 1: µβ = 1, µγ = 1

Case 2: µβ = 1, µγ = 10
0.16

0.1

0.14
0.12

0.08

0.1
0.06

0.08
0.06

0.04

0.04
0.02
0.02
0
−5

0
β−β0

0
−5

5

Case 3: µβ = 10, µγ = 1

0
β−β0

5

Case 4: µβ = 10, µγ = 10

0.25

0.4

0.2
0.3
0.15
0.2
0.1
0.1

0.05
0
−0.5

0
β−β0
USSIV

0
−0.5

0.5

Proj−SS

Robins

0
β−β0
Proj−AR

Partial−K

0.5

5%

Figure 7: Rejection Rates for tests of H0 : β = β0 are computed based on 10,000 Monte-Carlo
Trials. Weak instrument characterized by µ = 1 and strong instrument by µ = 10.

33

Power Curves when k = 4, n = 100, ρuX = 0.1, ρuW = 0.99, ρXW = 0 and ζ = 0.25
Case 1: µβ = 1, µγ = 1

Case 2: µβ = 1, µγ = 10

0.1
0.12
0.08

0.1

0.06

0.08
0.06

0.04
0.04
0.02

0.02

0
−5

0
β−β0

0
−5

5

Case 3: µβ = 10, µγ = 1

0
β−β0

5

Case 4: µβ = 10, µγ = 10

0.4

0.35

0.35

0.3

0.3

0.25

0.25
0.2

0.2

0.15

0.15
0.1

0.1

0.05

0.05

0
−0.5

0
β−β0
USSIV

0
−0.5

0.5
Proj−SS

Robins

0
β−β0
Proj−AR

Partial−K

0.5
5%

Figure 8: Rejection Rates for tests of H0 : β = β0 are computed based on 10,000 Monte-Carlo
Trials. Weak instrument characterized by µ = 1 and strong instrument by µ = 10.

34

Power Curves when k = 4, n = 100, ρuX = 0.99, ρuW = 0.1, ρXW = 0 and ζ = 0.25
Case 1: µβ = 1, µγ = 1

Case 2: µβ = 1, µγ = 10

0.4

0.8

0.35

0.7

0.3

0.6

0.25

0.5

0.2

0.4

0.15

0.3

0.1

0.2

0.05

0.1

0
−5

0
β−β0

0
−5

5

0
β−β0

Case 3: µβ = 10, µγ = 1

Case 4: µβ = 10, µγ = 10

0.4

0.8

0.35

0.7

0.3

0.6

0.25

0.5

0.2

0.4

0.15

0.3

0.1

0.2

0.05

0.1

0
−0.5

0
β−β0
USSIV

0
−0.5

0.5

Proj−SS

5

Robins

Proj−AR

0
β−β0
Partial−K

0.5

5%

Figure 9: Rejection Rates for tests of H0 : β = β0 are computed based on 10,000 Monte-Carlo
Trials. Weak instrument characterized by µ = 1 and strong instrument by µ = 10.

35

Power Curves when k = 4, n = 100, ρuX = 0.5, ρuW = 0.5, ρXW = 0 and ζ = 0.25
Case 1: µβ = 1, µγ = 1

Case 2: µβ = 1, µγ = 100
0.16

0.1
0.14
0.12

0.08

0.1
0.06
0.08
0.06

0.04

0.04
0.02
0.02
0
−5

0
β−β0

0
−5

5

Case 3: µβ = 100, µγ = 1

0
β−β0

5

Case 4: µβ = 100, µγ = 100

0.5
0.8
0.4
0.6

0.3

0.4

0.2

0.2

0.1
0
−0.5

0
β−β0
USSIV

0.5

Proj−SS

Robins

0
−0.5

0
β−β0
Proj−AR

Partial−K

0.5

5%

Figure 10: Rejection Rates for tests of H0 : β = β0 are computed based on 10,000 Monte-Carlo
Trials. Weak instrument characterized by µ = 1 and strong instrument by µ = 100.

36

Power Curves when k = 4, n = 100, ρuX = 0.1, ρuW = 0.99, ρXW = 0 and ζ = 0.25
Case 1: µβ = 1, µγ = 1

Case 2: µβ = 1, µγ = 100
0.14

0.1
0.12
0.08

0.1
0.08

0.06

0.06
0.04
0.04
0.02

0.02

0
−5

0
β−β0

0
−5

5

Case 3: µβ = 100, µγ = 1

0
β−β0

5

Case 4: µβ = 100, µγ = 100

0.5
0.8
0.4
0.6
0.3
0.4

0.2

0.2

0.1
0
−0.5

0
β−β0
USSIV

0.5

Proj−SS

Robins

0
−0.5

0
β−β0
Proj−AR

Partial−K

0.5

5%

Figure 11: Rejection Rates for tests of H0 : β = β0 are computed based on 10,000 Monte-Carlo
Trials. Weak instrument characterized by µ = 1 and strong instrument by µ = 100.

37

Power Curves when k = 4, n = 100, ρuX = 0.99, ρuW = 0.1, ρXW = 0 and ζ = 0.25
Case 1: µβ = 1, µγ = 1

Case 2: µβ = 1, µγ = 100

0.4
0.8

0.35
0.3

0.6

0.25
0.2

0.4
0.15
0.1

0.2

0.05
0
−5

0
β−β0

0
−5

5

Case 3: µβ = 100, µγ = 1

0
β−β0

5

Case 4: µβ = 100, µγ = 100

0.5
0.8
0.4
0.6
0.3
0.4

0.2

0.2

0.1
0
−0.5

0
β−β0
USSIV

0
−0.5

0.5

Proj−SS

Robins

0
β−β0
Proj−AR

Partial−K

0.5

5%

Figure 12: Rejection Rates for tests of H0 : β = β0 are computed based on 10,000 Monte-Carlo
Trials. Weak instrument characterized by µ = 1 and strong instrument by µ = 100.

38

Power Curves when k = 4, n = 100, ρuX = 0.5, ρuW = 0.5, ρXW = 0 and ζ = 0.5
Case 1: µβ = 1, µγ = 1

Case 2: µβ = 1, µγ = 10

0.16
0.2

0.14
0.12

0.15

0.1
0.08

0.1

0.06
0.04

0.05

0.02
0
−5

0
β−β0

0
−5

5

Case 3: µβ = 10, µγ = 1

0
β−β0

5

Case 4: µβ = 10, µγ = 10
0.6

0.4
0.35

0.5

0.3
0.4

0.25

0.3

0.2
0.15

0.2

0.1
0.1

0.05
0
−0.5

0
β−β0
USSIV

0
−0.5

0.5

Proj−SS

Robins

0
β−β0
Proj−AR

Partial−K

0.5

5%

Figure 13: Rejection Rates for tests of H0 : β = β0 are computed based on 10,000 Monte-Carlo
Trials. Weak instrument characterized by µ = 1 and strong instrument by µ = 10.

39

Power Curves when k = 4, n = 100, ρuX = 0.1, ρuW = 0.99, ρXW = 0 and ζ = 0.5
Case 1: µβ = 1, µγ = 1

Case 2: µβ = 1, µγ = 10

0.16
0.14
0.15

0.12
0.1

0.1

0.08
0.06

0.05

0.04
0.02
0
−5

0
β−β0

0
−5

5

Case 3: µβ = 10, µγ = 1

0
β−β0

5

Case 4: µβ = 10, µγ = 10
0.5

0.4
0.4
0.3
0.3
0.2

0.2

0.1

0.1

0
−0.5

0
β−β0
USSIV

0
−0.5

0.5

Proj−SS

Robins

0
β−β0
Proj−AR

Partial−K

0.5

5%

Figure 14: Rejection Rates for tests of H0 : β = β0 are computed based on 10,000 Monte-Carlo
Trials. Weak instrument characterized by µ = 1 and strong instrument by µ = 10.

40

Power Curves when k = 4, n = 100, ρuX = 0.99, ρuW = 0.1, ρXW = 0 and ζ = 0.5
Case 1: µβ = 1, µγ = 1

Case 2: µβ = 1, µγ = 10
0.8

0.4

0.7
0.6

0.3

0.5
0.4

0.2

0.3
0.2

0.1

0.1
0
−5

0
β−β0

0
−5

5

Case 3: µβ = 10, µγ = 1

0
β−β0

5

Case 4: µβ = 10, µγ = 10

0.5

0.8

0.4
0.6
0.3
0.4
0.2
0.2

0.1
0
−0.5

0
β−β0
USSIV

Proj−SS

0
−0.5

0.5
Robins

0
β−β0
Proj−AR

Partial−K

0.5
5%

Figure 15: Rejection Rates for tests of H0 : β = β0 are computed based on 10,000 Monte-Carlo
Trials. Weak instrument characterized by µ = 1 and strong instrument by µ = 10.

41

Power Curves when k = 4, n = 100, ρuX = 0.5, ρuW = 0.5, ρXW = 0 and ζ = 0.5
Case 1: µβ = 1, µγ = 1

Case 2: µβ = 1, µγ = 100

0.16
0.2

0.14
0.12

0.15

0.1
0.08

0.1
0.06
0.04

0.05

0.02
0
−5

0
β−β0

0
−5

5

Case 3: µβ = 100, µγ = 1

0
β−β0

5

Case 4: µβ = 100, µγ = 100

0.6
0.5

0.8

0.4

0.6

0.3
0.4
0.2
0.2

0.1
0
−0.5

0
β−β0
USSIV

0
−0.5

0.5
Proj−SS

Robins

0
β−β0
Proj−AR

Partial−K

0.5
5%

Figure 16: Rejection Rates for tests of H0 : β = β0 are computed based on 10,000 Monte-Carlo
Trials. Weak instrument characterized by µ = 1 and strong instrument by µ = 100.

42

Power Curves when k = 4, n = 100, ρuX = 0.1, ρuW = 0.99, ρXW = 0 and ζ = 0.5
Case 1: µβ = 1, µγ = 1

Case 2: µβ = 1, µγ = 100

0.16
0.14
0.15

0.12
0.1

0.1

0.08
0.06

0.05

0.04
0.02
0
−5

0
β−β0

0
−5

5

Case 3: µβ = 100, µγ = 1

0
β−β0

5

Case 4: µβ = 100, µγ = 100

0.6
0.8

0.5

0.6

0.4
0.3

0.4

0.2
0.2

0.1
0
−0.5

0
β−β0
USSIV

0
−0.5

0.5
Proj−SS

Robins

0
β−β0
Proj−AR

Partial−K

0.5
5%

Figure 17: Rejection Rates for tests of H0 : β = β0 are computed based on 10,000 Monte-Carlo
Trials. Weak instrument characterized by µ = 1 and strong instrument by µ = 100.

43

Power Curves when k = 4, n = 100, ρuX = 0.99, ρuW = 0.1, ρXW = 0 and ζ = 0.5
Case 1: µβ = 1, µγ = 1

Case 2: µβ = 1, µγ = 100

0.4

0.8

0.3

0.6

0.2

0.4

0.1

0.2

0
−5

0
β−β0

0
−5

5

Case 3: µβ = 100, µγ = 1

0
β−β0

5

Case 4: µβ = 100, µγ = 100

0.6
0.8

0.5
0.4

0.6

0.3
0.4
0.2
0.2

0.1
0
−0.5

0
β−β0
USSIV

0
−0.5

0.5
Proj−SS

Robins

0
β−β0
Proj−AR

Partial−K

0.5
5%

Figure 18: Rejection Rates for tests of H0 : β = β0 are computed based on 10,000 Monte-Carlo
Trials. Weak instrument characterized by µ = 1 and strong instrument by µ = 100.

44

References
Angrist, J. and Krueger, A. B. (1992). The Effect of Age at School Entry on Educational Attainment:
An Application of Instrumental Variables with Moments from Two Samples. Journal of American
Statistical Association, 87: 328–336.
Angrist, J. and Krueger, A. B. (1995). Spilt-Sample Instrumental Variables Estimates of the Return
to Schooling. Journal of Business and Economics Statistics, 13: 225–235.
Chaudhuri, S. (2007). Testing of Hypotheses in Partially Identified Linear Instrumental Variables
Model. Job-Market Paper.
Choi, I. and Phillips, P. C. B. (1992). Asymptotic and Finite Sample Distribution Theory for
IV Estimators and Tests in Partially Identified Structural Equations. Journal of Econometrics,
51: 113–150.
Dufour, J. M. (1997). Some Impossibility Theorems in Econometrics with Applications to Structural
and Dynamic Models. Econometrica, 65: 1365–1388.
Dufour, J. M. and Jasiak, J. (2001). Finite Sample Limited Information Inference Methods for
Structural Equations and Models with Generated Regressors. International Economic Review,
42: 815–843.
Dufour, J. M. and Taamouti, M. (2005). Projection-Based Statistical Inference in Linear Structural
Models with Possibly Weak Instruments. Econometrica, 73: 1351–1365.
Guggenberger, P. and Smith, R. (2005). Generalized Empirical Likelihood Estimators and Tests
under Partial, Weak and Strong Identification. Econometric Theory, 21: 667–709.
Hall, A. R., Rudecusch, G. D., and Wilcox, D. W. (1996). Judging Instrumental Relevance in
Instrumental Variables Estimation. International Economic Review, 37: 283–298.
Kleibergen, F. (2002). Pivotal Statistics for Testing Structural Parameters in Instrumental Variables
Regression. Econometrica, 70: 1781–1803.
Kleibergen, F. (2004). Testing Subsets of Parameters In The Instrumental Variables Regression
Model. The Review of Economics and Statistics, 86: 418–423.
Mikusheva, A. (2006). Robust Confidence Sets in the Presence of Weak Instruments. Department
of Economics, Harvard University.
Moreira, M. J. (2003). A Conditional Likelihood Ratio Test for Structural Models. Econometrica,
71: 1027–1048.
Phillips, P. C. B. (1989). Partially Identified Econometric Models. Econometric Theory, 5: 181–240.
45

Robins, J. M. (2004). Optimal Structural Nested Models for Optimal Sequential Decisions. In Lin,
D. Y. and Heagerty, P., editors, In Proceedings of the Second Seattle Symposium on Biostatistics.
New York: Springer.
Shea, J. (1997). Instrument Relevance in Multivariate linear models: A simple measure. The Review
of Economics and Statistics, LXXIX: 348–352.
Staiger, D. and Stock, J. H. (1997). Instrumental Variables Regression with Weak Instruments.
Econometrica, 65: 557–586.
Stock, J. H. and Yogo, M. (2005). Testing for Weak Instruments in Linear IV Regression. In
Andrews, D. W. K. and Stock, J. H., editors, Identification and Inference for Econometric Models:
Essays in Honor of Thomas Rothenberg, pages 80–108. Cambridge University Press.
Wang, J. and Zivot, E. (1998). Inference on a Structural Parameter in Instrumental Variables
Regression with Weak Instruments. Econometrica, 66: 1389–1404.
Zivot, E. and Chaudhuri, S. (2007). The Distribution of Shea’s Partial R2 Statistic in the Presence
of Weak Instruments. Unpublished.
Zivot, E., Startz, R., and Nelson, C. (1998). Valid Confidence Intervals and Inference in the Presence
of Weak Instruments. International Economic Review, 39: 1119–1144.
Zivot, E., Startz, R., and Nelson, C. (2006). Inference in Weakly Identified Instrumental Variables
Regression. In Corbae, D., Durlauf, S. N., and Hansen, B. E., editors, Frontiers in Analysis and
Applied Research: Essays in Honor of Peter C. B. Phillips, pages 125–166. Cambridge University
Press.

46

