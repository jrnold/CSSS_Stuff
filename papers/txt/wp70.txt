1

The Effect of Weather Forecast Uncertainty Visualization on
Decision Making
Limor Nadav-Greenberg, Susan L. Joslyn and Meng U. Taing¹
University of Washington, Seattle

Working Paper no. 70
Center for Statistics and the Social Sciences
University of Washington
January 16, 2007

¹Limor Nadav-Greenberg is a graduate student, Department of Psychology, University of
Washington, Box 351525, Seattle WA 98195-1525. E-mail: limorn@u.washington.edu,
Susan L. Joslyn is a Senior Lecturer Department of Psychology, University of Washington, Box
351525, Seattle WA 98195-1525. E-mail: susanj@u.washington.edu; Web: http://faculty.
washington.edu/susanj/, Meng U. Taing, is a graduate student, Department of Psychology,
University of South Florida, Tampa, FL 33620-7200. E-mail: mtaing@mail.usf.edu. This
research was supported by the DOD Multidisciplinary University Research Initiative (MURI)
program administered by the Office of Naval.

2

Abstract
Modern day weather forecasting relies heavily on numerical weather and climate models that
make weather predictions. These predictions are accompanied by varying amounts of
uncertainty. The studies reported here investigated the effect of improved visualizations on the
understanding and use of forecast uncertainty. Participants spanned the range of expertise from
novices in weather forecasting to experienced professional forecasters. We investigated three
visualizations based on the 80 percent predictive interval, which were: (a) a chart showing the
amount of uncertainty, (b) a chart showing the worst case scenario, and (c) box plots of likely
wind speeds. Participants used these visualizations to determine the relative uncertainty in the
forecast, forecast wind speed and decide whether to post a high wind advisory. The results for
novices and professional forecasters were similar. The uncertainty chart and box plots improved
performance and understanding in different ways, suggesting that each visualization was optimal
for different forecasting tasks. The worst case scenario introduced bias without any benefits.

3
Modern day weather forecasters rely heavily on numerical weather and climate models that
make weather predictions by transforming present into future weather conditions according to
the known principles of atmospheric physics. However, these forecasts are inherently uncertain
due to uncertainties of the initial state of the atmosphere and the computational representation of
the equations of motion. The methods for assessing numerical model forecast uncertainty have
been available for over 30 years (Epstein, 1969; Leith, 1974). Recent significant improvements
in these methods (Raftery, Gneiting, Balabdaoui & Polakowski, 2005; Gneiting & Raftery, 2005;
Sloughter, Raftery, Gneiting & Fraley, in press) have lead to the development of several new
uncertainty products. The explicit information about forecast uncertinaty provided by such
products could benefit many forecasting decisions (Pielke, 1999; Keith, 2003; National Research
Council, 2006). However, such products are currently under used by professional forecasters
(Joslyn & Jones, in press) and with the exception of probability of precipitation, forecast
uncertainty is rarely communicated to the general public (NRC, 2006).
There are a number of possible explanations for the current under use of forecast
uncertinaty information. Uncertainty products may be under used in part because of the
overwhelming amount of information available to the forecaster, much of it on the Internet
(Pliske, Klinger, Hutton, Crandall, Knight & Klein, 1997). More information is available than
can be incorporated in a given forecast. The forecaster must sift through the available
information to identify which sources are most relevant and useful. As such, ease of access may
be a critical factor in determining whether uncertainty information is included. Usability research
suggests that information will be incorporated to the degree that it is easy to use, easy to
understand and helps people reach existing goals (Norman, 1986; Endsley, Bolté & Jones, 2003).
Even relevant information presented in a format that is not compatible with the users’ goals and

4
information processing tendencies may be ignored (Heath & Luff, 2000). Thus, existing
uncertainty products may be ignored in part because they were not designed with any specific
user groups in mind and reflect instead complex theoretical constructs important to the creators.
Another possible explanation for under use of uncertainty information is that it is inherently
difficult for people to understand. A large body of psychological research suggests that people
have trouble reasoning with uncertainty information and systematically deviate from the
normative interpretation of the information (Tversky & Kahneman, 1979, 1992; Camerer & Ho,
1994; Gonzalez & Wu, 1999). Similar findings were observed in professional decision makers
such as doctors and lawyers (Koehler, Brenner & Griffin, 2002). Although professional weather
forecasters fare somewhat better when estimating uncertainty (Murphy & Winkler, 1974a,
1974b, 1977; Koehler, Brenner & Griffin, 2002), there is recent evidence that suggests the
influence of cautiousness under some circumstances (Brown & Murphy, 1987) and an overforecasting bias when safety is an issue (Keith, 2003).
The difficulty that people have understanding uncertainty is compounded in the weather
forecast context because the meaning of probabilistic forecasts is debated, even within the
scientific community (deElia & Laprise, 2005). According to one interpretation an 80% chance
of high winds means that there will be high winds 80% of the times that high winds are
forecasted. Another way of thinking about this is to say that there will be high winds in 80% of
similar atmospheric situations (i.e., days with weather conditions like they are today). Notice that
one problem with this approach is defining what counts as a “day like today”. This is referred to
as the reference class problem. For weather forecasts, the reference class is an abstract and often
unstated, hence debatable, construct. Unlike a coin toss, for which a 50% chance of heads can be
interpreted as the expectation of 50% heads over repeated coin tosses, a 50% chance of rain

5
refers ostensibly to a single event, a particular day. It is difficult to regard “tomorrow” as a
repeated event.
The factors enumerated here (i.e., accessibility, compatibility with user information
processing requirements and goals and conceptualization of uncertainty), are likely to be crucial
in determining whether uncertainty products will be used. The goal of the research presented
here was to test visualizations of forecast uncertainty that were designed to address each of these
factors.
To begin with, we selected a conceptualization of forecast uncertainty that may by-pass
some of the reference class confusion described above. It is the 80% predictive interval, which
can be interpreted as error bars around the deterministic forecast. Using the example of wind
speed, the 80% predictive interval is a range of wind speeds that will include the observed wind
speed with an 80% probability. As such, it can be conceptualized as confidence in the forecast,
something that may be more natural to many weather forecasters (deElia & Laprise, 2005). The
80% predictive interval is derived from a probability distribution of values for the weather
parameter of interest (Gneiting & Raftery, 2005), in this case wind speed. The probability
distribution is inferred from the percent agreement between ensemble members.
We created three visualizations based on the 80% predicative interval. According to
private meteorological sector providers, users often want just the worst-case scenario (NRC,
2006). The statistical equivalent of the worst-case scenario for the wind speed example above is
the wind speed at the upper bound of the predictive interval. If the wind speed at the upper bound
could result in significant safety or economic hazards, precautionary action may be warranted
even though the probability of observing a wind speed that high is only 10%. The upper bound
was presented in a chart, color-coded for wind speed.

6
Another potentially useful visualization is a direct representation of the amount of
uncertainty in the forecast. Our previous research suggested that forecasters engage in elaborate
comparisons between observed and predicted values in an attempt to identify areas where the
numerical model predictions are less reliable (Joslyn & Jones, in press). A direct representation
of such information would meet these needs and reduce information processing load. We created
a chart which presented the uncertainty information associated with the specific forecast. Degree
of uncertainty was derived from the range of wind speeds in the 80% predicative interval. As the
range increased so did the uncertainty in the forecast because the actual wind speed could be
anywhere within that wide range or below the median (Gneiting, Balabdaoui & Raftery, in press
or Gneiting & Raftery, 2005).
The third and final visualization, unlike the first two representations, was not a chart but a
box plot that delineated the median and the upper and lower bounds for a specific location.
The box plot is common in the world of statistics but is a novel form of representation in
atmospheric science. It had the advantage of incorporating all of the relevant information in a
single representation increasing ease of access. It showed the amount of uncertainty in the
forecasts (range) as well as the worst-case scenario (upper bound).
We presented these three uncertainty visualizations, along with a median wind speed
chart, to two groups of participants, undergraduate atmospheric science students (Experiment 1)
and professional forecasters (Experiment 2). We asked participants to use the information to
make a single value, deterministic wind speed forecast, and to decide whether to issue a high
wind advisory indicating that winds were expected to exceed 20 knots. We also asked them to
assess the amount of uncertainty in the forecast. Then, we analyzed the differences in
participants’ responses when viewing the three different visualizations.

7
Experiment 1
Method
Participants
Participants were 26 students from the Atmospheric Science Department (ATMS) at the
University of Washington. They were paid $20 in cash for participation.
Stimuli
Participants were presented with 10 hypothetical wind speed forecasts based on actual
forecasts from the Fifth-Generation Pennsylvania State University −National Center of
Atmospheric Research Mesoscale Model (MM5) - a widely used computer model produced by
the University of Washington. The forecasts were for three locations in Western Washington and
included wind speed predictions for 48 hours into the future. The forecasted wind speeds ranged
between 6 and 30 knots.
Participants received information about the 80% predicative interval for each forecast
(and each location). Three aspects of the predictive interval were provided in all three conditions:
the median wind speed (the wind speed at the 50th percentile of the predictive interval), the upper
bound (i.e., the wind speed at the 90th percentile of the distribution) and the range of wind speeds
(i.e., the difference between the median and the upper bound wind speeds1). All three groups
received the median chart, color-coded with warmer colors indicating higher wind speeds (see
Appendix A for example). Participants were told to regard the median wind speed as the
deterministic forecast.
Along with the median chart, participants received an additional experimental display,
which allowed them to derive information about both the upper bound and the range. Participants

8
were told that the upper bound was the highest likely wind speed and that the range of wind
speeds represented the uncertainty in the forecast: as the range increased so did the uncertainty
because the actual wind speed could be anywhere within that wide range or below the median. In
some conditions this information was directly represented in the visualization; in others it could
be derived from information that was directly represented.
The three experimental conditions varied in manner in which they represented the upper
bound and the range. In one condition, in addition to the median chart, participants received the
upper bound chart, showing the wind speeds at the 90% upper bound of the probability
distribution. This chart was color-coded, with warmer colors indicating higher wind speeds,
using the same color scale as the median chart (see Appendix B). In this condition, the range,
representing the uncertainty level in the forecast, could be obtained by subtracting the median
wind speed shown on the median chart from the upper bound wind speed (for the same location)
shown on the upper bound chart.
In the second condition, in addition to the median chart, participants received a margin of
error chart, which directly indicated the range of wind speeds between the median and the upper
bound. Warmer colors indicated a wider range of wind speeds or greater uncertainty in the
forecast (see Appendix C). A different color scale was used on this chart to distinguish it from
the predictive wind speed chart (i.e., the median chart). In this condition, the upper bound could
be obtained by adding the median wind speed (from the median chart) to the range of wind
speeds (from the margin of error chart).
In the third condition, in addition to the median chart, participants received a box plot
graph (see Appendix D), for each forecast location. The box plot showed the median wind speed
as a vertical line inside a box. The box itself included the 50% predictive interval. The upper and

9
the lower bounds were represented by vertical lines at the end points of a horizontal line on each
end of the box. The value of the upper bound could be obtained by reading the wind speed below
the horizontal line. The range, representing the uncertainty level in the forecast, was obtained by
computing the difference between the median value and the upper bound value.
Design
Participants were randomly assigned to one of the three experimental conditions (upper
bound chart, margin of error or box plot). The wind speed data was identical for all three
conditions. The only difference among conditions was the format in which it was displayed.
Procedure
Participants were tested in groups of one to three participants per session. Participants in
the same session were assigned to the same condition. Each session lasted approximately one
and a half hours.
Training phase. All participants were first instructed in how to use the new visualizations
to insure that experimental responses were not due to lack of understanding. The training phase
began with an informed consent procedure. Then, the experimenter read aloud the training script
that was also shown on the computer screen in front of each participant. The training script
explained how to read the median chart and how to obtain the upper bound and range using the
experimental visualizations. Instructions about how to obtain the upper bound and the range
varied slightly depending on condition (for exact instructions see Appendixes B, C and D).
Participants were then given illustrated examples of how to use the charts to read the median
wind speed, upper bound, and the range.
Practice phase. Following the tutorial, participants worked through a series of practice
trials to ensure that they understood how to derive the median, upper bound, and range of wind

10
speeds from the visualizations. They were asked to derive these values for several forecast dates
for three locations that would later become the test forecast sights. Participants did not practice
any questions that required inferential reasoning or judgment such as assessing the uncertainty in
the forecast or deciding whether to post a high wind advisory. Participants’ answers were
compared with the actual data and immediate feedback was provided. If the answers were
correct, the message “Good!” appeared on the screen. If one of the answers was more than 2
knots off, a message was displayed indicating the question number and the correct answer for it
(e.g., “The correct answer for question number 1 is 16 knots. If you have any questions, please
ask the experimenter”). When participants answered 24 questions correctly (four questions on
each one of the three locations, for two separate forecast dates) they advanced to the test phase.
The practice phase included 120 questions, however no participant needed more than 36
questions to reach criterion. Participants were encouraged to ask questions throughout the
training and the practice phases, but were not allowed to do so during the test phase.
At the end of the practice phase participants were reminded that the range of wind speeds
indicated the level of uncertainty. They were told that there were additional questions they would
be asked in the test phase for which they were given no practice because they were questions that
did not have correct answers.
Test Phase. During the test phase, 10 different forecasts were presented one at a time.
The dates of the forecasts and the order in which they were presented were identical across
conditions. For each forecast, participants answered the same questions they answered during the
practice phase and some additional ones. The new questions asked participants to estimate the
uncertainty in the forecast, make their own forecasts and decide whether to issue a high wind
advisory to warn boaters of dangerously high winds. The guideline for the high wind advisory

11
was to post the small craft advisories when winds were expected to be 20 knots or greater in a
given area. Participants were also asked to indicate how sure they were about their decisions2
(see Appendix E for a complete list of the test questions). Participants were instructed to reach
the best decision they could, based on the information provided.
When all 10 forecasts were completed, participants answered a series of questions asking
them to assess the usability of the products as well as the quality of their own forecast decisions.
The first two questions asked participants to rate understandability of each graphic. The second
two questions asked participants to rate the ease of use of each graphic. Participants indicated
their rating with pull down menus showing values between 1 (“very difficult”) and 10 (“very
easy”). The next three questions asked participants to estimate the number of wind speed
forecasts out of 10 they thought were within 2, 5 and 10 knots of the observed wind speed. Then
they were asked how many of the 10 wind speed advisory decisions were appropriate. Finally
they were asked whether advisory posting errors, were likely to be misses or false alarms. When
the questionnaire was completed, participants were paid and debriefed.
Apparatus
The practice and test materials were presented on desktop computers with a 17-inch
monitor and a standard mouse and keyboard. Three workstations were located in the same room,
facing different directions. The practice and test were programmed using Microsoft Excel Office
and all the data entered by participants was automatically recorded in an Excel file. Participants
were provided with scratch paper for making calculations during the practice and test phases.
Results
We analyzed participants’ ability to read the predicted wind speeds, estimate the
uncertainty level associated with the forecast, make their own wind speed forecasts and decide

12
whether to post high wind advisories. Because of the potential impact on the subsequent
analyses, we began by analyzing participants’ ability to read the median wind speed.
First we computed an average reading error for each participant, by calculating the
mean differences between their readings of the median wind speeds and the actual wind speeds
displayed. Because the box plot graphs included information about the median wind speed, we
wanted to see whether accuracy in this condition was different form accuracy in the other two
conditions, in which participants could read the median wind speed only from the color-coded
median chart. An analysis of variance was then performed to test differences in reading accuracy
among the three visualization conditions. Visualization had a significant effect on reading
accuracy (F (2, 23) = 5.47, p < .02). Post-Hoc Tukey’s HSD revealed that the average reading
error made by participants in the box plot condition (M = -.02, SD = .45) was significantly
smaller than that made by participants in both the margin of error (M = .61, SD = .31; p < .02)
and upper bound (M = .52, SD = .49; p < .04) conditions. The average reading errors in the
margin of error and upper bound were not significantly different from each other3. SingleSample T-tests for each condition revealed that the box plots were accurately read (average
reading errors were not statistically different than zero, t (7) = .13, p = .90). However, reading
the median wind speed using the color-coded chart resulted in errors that were significantly
greater than zero (margin of error: t (8) = 5.85, p < .01; upper bound: t (8) = 3.22, p < .02).
Next we examined the uncertainty ratings for each forecast. Recall that participants were
told that uncertainty was reflected in the width of the range of likely wind speeds. Thus
participants’ interpretation of the relative uncertainty in the forecasts could be assessed by
examining the correlations between uncertainty ratings and the actual wind speed ranges.

13
Correlations were calculated individually for each participant and then rank-ordered. The highest
correlations were calculated for participants in the margin of error condition, with a median
correlation of .89 (IQR = .86 - .93). The next highest correlations were calculated for those in the
box plot condition (median = .82, IQR = .79 - .85) and the smallest correlations were calculated
for those in the upper bound condition (median = .69, IQR = .64 - .70). The median correlation
in the margin of error condition was significantly greater than that in the upper bound condition
(U = 18, p < .05). The median correlation in the box plot condition was also significantly greater
than the median correlation in the upper bound condition (U = 63, p < .02). The comparison
between the margin of error and the box plot condition approached significance (p = .07). It
should be noted that the high median correlation in the margin of error condition was observed
despite the fact that two out of the nine participants in the margin of error group had very low
correlations (.11 and .06) and were kept in the analysis. This suggests that the margin of error
may be the optimal format for deriving relative uncertainty although it may be challenging for
some people. We will return to this issue in the discussion section.
After assessing the uncertainty level in the forecast, participants were asked to forecast a
single wind speed (deterministic forecast). We were interested in whether there would be
differences in wind speed predictions due to visualization format. Because participants were told
to regard the median wind speed as the deterministic forecast, the influence of information
presented in the uncertainty visualizations will be observed in the deviation from this value. We
calculated the difference between participants predicted wind speeds, for each location, and their
reading of the median wind speed. Then we calculated the average deviation for each participant.
In the upper bound condition, participants’ deviations were significantly greater than zero
(M = 2.10, SD = 2.55; t (8) = 2.48, p < .04). In other words, when given the upper bound chart,

14
participants made significantly higher wind speed predictions than the median wind speed as
they read it. However, the difference was not statistically significant (i.e., greater than zero) in
either the margin of error (M = 1.55, SD = 2.38) or the box plot (M = 1.43, SD = 1.97)
conditions. Recall that the underlying forecasts were identical in all three conditions. Thus, this
analysis suggests that the upper bound visualization, increased the positive bias of the subjective
forecasts from the perceived deterministic forecast.
Finally, we asked whether display format influenced participants’ decisions about
whether or not to post a high wind advisory. Participants were instructed to post an advisory if
wind speeds were expected to be equal to or greater than 20 knots. To examine these decisions
we grouped the forecasts into three categories depending on the likelihood of high winds in the
materials that participants were given. Situations in which the median wind speed was 20 knots
or greater were classified as “high winds”, situations in which median wind speeds were between
15 and 20 knots, were classified as “medium winds” and situations in which median wind speeds
were below 15 knots were classified as a “low winds”. Table 1 shows the percentage of
advisories posted in these three categories in each condition. When high winds were highly
likely, participants in the box plot condition posted the highest percentage of advisories
(98.4%).The same pattern was observed in the medium winds. Because more than half of the
cases in the medium winds category were cases in which the median wind speed was above 18.2
knots, it could be argued that posting advisories in those cases was justified. As demonstrated by
Figure 1, the percentage of advisories posted in the box plot condition consistently increased
with the likelihood of high winds predicted by the computer model. Thus, participants viewing
the box plot tended to post more advisories when these warnings were justified.
Exit Questions Analysis

15
To assess the usability and confidence inspired by the graphics, we analyzed participant’s
answers to the seven exit questions. Recall that participants were asked to rate the ease of use of
each graphic on a scale of 1 to 10 (1 – being “very difficult” and 10 being “very easy”). Table 2
summarizes participants’ responses to the exit questions. A Post-Hoc Tukey’s HSD revealed that
the box plot was rated as significantly easier to use than the margin of error chart (p < .02). No
other group differences were significant.
We also examined participants’ estimate of the accuracy of their own forecasts4. Recall
that participants were asked to estimate how many of their forecasts were within 2, 5 and 10
knots of the actual wind speeds. Participants in the margin of error condition estimated fewer
correct forecasts on all three questions, than did participants in the other two conditions (see
Table 2 for means and standard deviations). Post-Hoc Tukey’s HSD revealed that similarly,
participants in the margin of error condition believed they made significantly fewer correct
advisory decisions than did participants in either the upper bound (p < .04) or the box plot
conditions (p < .03). Thus participants in the margin of error condition demonstrated increased
awareness of the inherent uncertainty and that was also reflected in decreased confidence. Note
that it was not possible to estimate actual accuracy as these were hypothetical forecasts. None the
less, these findings are particularly interesting given that the data provided were identical in all
three conditions, including the range, meant to indicate uncertainty.
Discussion Experiment 1
Experiment 1 revealed that the box plot and the margin of error chart were
optimal for different forecasting tasks. People using the box plot format posted more high wind
advisories when high winds were likely and yet their wind speed forecast did not have the high

16
bias that was observed in the upper bound condition. Although it might make sense to “overforecast” deterministic wind speeds when safety is an issue (as it was in this high wind advisory
task), if the bias was due to safety concerns alone it would occur in all three conditions.
However, we observed significantly higher wind speed forecasts only in the upper bound
condition suggesting that the bias was at least partially due to the display format. Emphasizing
the upper bound may have acted as a high anchor (for other examples, see Tversky & Kahneman,
1974; Chapman & Johnson, 2002), drawing the deterministic forecast up in that condition. The
fact that the box plot included both the upper and lower bounds may have counteracted this
tendency because it provided an anchor for both ends. The box plot format was also optimal in
terms of usability. It was read with greatest accuracy and was rated easiest for participants to use.
The margin of error chart was optimal for detecting the relative uncertainty in the
forecast. This was evidenced in the uncertainty ratings. Correlations between uncertainty rating
and the range, upon which participants were instructed to base their uncertainty estimates, were
significantly higher in the margin of error condition. In addition, people in the margin of error
condition estimated that they had made significantly fewer correct forecasts than those in the
other two conditions. This could have been due to the fact that forecast uncertainty was
emphasized in this condition leading to an increased awareness of the inherent uncertainty in the
forecast. Alternatively, it could also have been due to the difficulty in using and understanding
this format. Recall that the margin of error chart was rated as least easy to use. It could be argued
that the difficulty participants experienced when extracting information from this chart
undermined their confidence in the accuracy with which they were reading the chart. However,
given the fact that the uncertainty ratings were more accurate in this condition, it is our belief
that the decreased (and perhaps more realistic) degree of confidence in the forecast was driven

17
primarily by the greater awareness of the uncertainty involved in the forecast. This result
suggests that the margin of error chart might be the best format for presenting uncertainty
information to most people. Enhanced exposure and practice using the chart may help to
overcome the difficulty in use observed here.
Experiment 2
Experiment 1 revealed an advantage for the box plot and margin of error display formats,
depending on the forecast task. However, participants in Experiment 1 were novice weather
forecasters. It may have been that the advantages observed in this study could only be realized by
those with little forecasting experience. Perhaps those with greater forecasting experience would
not benefit to the same degree or would have different responses to the visualizations for some
other reason. The second experiment was conducted to investigate whether the same pattern of
results would be observed among professional forecasters.
Method
Participants
Nine professional weather forecasters from the National Oceanic and Atmospheric
Administration in Seattle volunteered to participate in this study. They were not compensated for
participation.
Design
Because of the small number of participants, a within-participants design was used. Each
participant was provided with different wind speed forecasts in all three formats that were used
in Experiment 1: upper bound, margin of error and box plot. To counteract potential order
effects, conditions were presented in two different orders. In the first version the order was: box
plot, margin of error and upper bound. In the second version the order was: upper bound, margin

18
of error and box plot. Unlike Experiment 1, the wind speed forecasts were different in each
condition so that participants did not see the same forecast more than once.
Stimuli
Participants received nine wind speed forecasts for the same three locations used in
Experiment 1. Although the forecasts were different for each condition they had similar median
wind speeds and ranges. See Appendix F for the specific forecasts we used (medians, means,
ranges and upper bound wind speeds).
Procedure
Participants were trained and tested individually. The training, practice, and test phases
were identical to those used in Experiment 1, except for the following differences: First, each
participant was trained, practiced and tested three times, once for each visualization format. The
median was introduced at the beginning of the first training phase. The second difference was
that, the practice phases were limited to an overall number of six forecasts for each condition
(two for each location) instead of practice to criterion as was the case in the first experiment. The
answers to practice questions were presented at the end of each practice session so that
participants could compare their responses with the correct answers and request clarification
when needed. Thirdly, in addition to the exit questions that followed each test in Experiment 1,
there was another final semi structured interview to get forecasters’ feedback on the products.
Apparatus
The test was presented on laptop computer with a 15-inch monitor. The test was
programmed using Microsoft Office Word, and all the data entered by participants was recorded
automatically.
Results

19
Again, we began by analyzing participants’ accuracy in reading the median wind speed,
using the same procedure to calculate average reading error per participant. As in Experiment 1,
the smallest reading errors were detected in the box plot condition (M = -.004, SD = .20),
followed by the margin of error condition (M = .50, SD = .60) and finally by the upper bound
condition (M = 1.15, SD = .73). A within-participants analysis of variance on average reading
errors suggested that there were significant differences among conditions F (2, 14) = 10.03, p <
.01. Pair-wise comparisons using Tukey’s HSD test with p set at .05, revealed that reading errors
in the box plot condition were significantly smaller than reading errors in the upper bound
condition. No other group differences were statistically significant.
To investigate the effect of visualization on uncertainty ratings, we calculated
correlations between participants’ uncertainty ratings and the model predicted ranges. As with
Experiment 1, the highest correlations were observed in the margin of error condition (median =
.78, IQR = .52 - .86), followed by the upper bound (median = .62, IQR = .37 - .75) and the box
plot (median = .42, IQR = .27 - .63). Because of the within-participants design of this study, we
conducted a Wilcoxon T-test to determine whether differences by condition were significant. We
computed a difference score for each pair of the three correlations obtained with the three
visualizations5 and then rank-ordered them. As with Experiment 1, the correlations between
uncertainty ratings and the range were significantly greater when participants used the margin of
error chart compared to when they used the box plot (T = 4, p <.05). Neither of the other two
comparisons reached significance. Thus, professional forecasters, like the students, were better
able to estimate the relative uncertainty in the forecast when using the margin of error chart.

20
Next we examined wind speed predictions. As with Experiment 1, we subtracted
participants’ forecasted wind speed from the median given by the computer model, to create a
deviation score. In Experiment 2, the mean deviations in all three conditions were significantly
greater than zero, (margin of error: M = 2.10, SD = 1.93; t = 3.07, p < .02; upper bound: M =
2.21, SD = 1.89; t = 3.30, p < .02; box plot: M =1.68, SD = 1.70; t = 2.97, p < .02). This suggests
that the professional forecasters increased their wind speed predictions over the median provided
by the computer model regardless of the visualization used. As with Experiment 1, the highest
mean deviation was observed in the upper bound condition, although it was not significantly
higher than the mean deviations in the other two conditions.
Finally we examined participants’ decisions about whether or not to post a high wind
advisory. Recall that participants were instructed to post an advisory if they thought wind speeds
would equal or exceed 20 knots. To examine these decisions, we used the same categorization of
high, medium, and low expected wind speeds as were used in Experiment 1. For all three
conditions, participants posted advisories 100% of the time when the median wind speed was 20
knots or greater (See Table 3). For the medium winds (wind speeds between 15 – 20 knots),
professional forecasters, regardless of condition, posted advisories about half of the time. Very
few advisories were posted in the low winds situations. Participants posted advisories slightly
less often in the box plot condition than in the upper bound and margin of error conditions, in the
medium and low winds situations. Thus, for decisions to post warning advisories, professional
forecasters did not appear to be influenced by visualization. They posted advisories in all of the
high winds situations and withheld advisories in most of the low winds situations.
Exit Questions Analysis

21
As with Experiment 1, the main difference due to visualization format was in ease of use
(see Table 4 for means and standard deviations). Professional forecasters, like the students, rated
the color-coded charts as more difficult to use than the box plot chart (Paired-Sample T-tests
approached significance). Examination of participants’ estimations of the accuracy of their
forecasts revealed that similar to Experiment 1, participants expressed the highest confidence in
their forecasts when they used the box plot chart and the least confidence when they used the
margin of error chart. Again, it was not possible to verify these forecasts as the situations were
hypothetical. In addition, participants in the box plot condition expressed more confidence in
their decisions to post warning advisories compared to participants in the other two conditions.
Discussion Experiment 2
The basic results of Experiment 1 were replicated among the professional forecasters
tested in Experiment 2. The box plot format was read most accurately and was rated as the
easiest chart to use. These results are particularly important given the within-participants design
used in this experiment, which allowed participants to make relative evaluations of the usability
of each display. As we found before, the margin of error chart was optimal for detecting relative
uncertainty in the forecast.
Unlike the students in the first experiment, the professional forecasters forecasted wind
speeds that were significantly higher than the median forecast in all three conditions. Debriefing
revealed that the forecasters may have been consciously counteracting perceived bias in the
computer model. However because the professional forecasters saw all three formats, this high
bias may have been due to carryover effects. Exposure to the upper bound condition, (which led
to high bias among students), may have influenced the professional forecasters when it was
presented first.

22
Among professional forecasters there were no differences by condition in the wind
advisory task. Moreover the professional forecasters had a more decisive pattern of posting.
They all chose to post an advisories when high winds were likely, and posted about half of the
time in medium expected wind speeds. Very few advisories were posted when high winds were
unlikely. Thus, forecasting experience may override any effects of format for this task.
Conclusions
The two experiments reported here sought to determine the optimal visualization format
for communicating information about weather forecast uncertainty. We discovered that the
optimal visualization varies by forecast task. For extracting precise information, the box plot was
the optimal format of the three investigated here. The margin of error chart was best for deriving
relative uncertainty in the forecast. The format for which we have the most concerns was the
upper bound chart, which introduced a high bias in the wind speed forecasts among the student
forecasters.
In both experiments people read wind speeds more accurately when viewing the box
plots compared to the color-coded chart formats. Moreover, although this format is novel in the
weather forecasting domain, it was rated easiest to use by both user groups, suggesting that it
might be preferred in real world contexts in which information overload is a factor and ease of
access is key. The reason for this is clear: Extracting information from the box plot required
fewer steps. Using the box plot, forecasters simply read the wind speeds shown directly below
the distinct lines of the display. The color-coded charts on the other hand required identifying the
color at the geographic location on the chart, matching it to the color on the legend and reading
off the corresponding wind speed. This process involved more steps and hence there was greater
potential for error. In addition it was dependent on identifying the color and then matching it to

23
the color on the legend. This process could be a problematic when subtle differences are shown
or when the user is colorblind. However, color coded charts have other advantages such as
showing overall weather patterns, information that is useful at stages in the forecasting process
not directly tested in this experiment. It is possible that improvements to the charts could be
made to overcome the difficulty of use identified here. Curser generated numeric displays (popup values) would probably be a useful addition.
The box plot display had other advantages as well. It appeared to be optimal for threshold
forecast decisions, especially for novice users. Recall that the professional forecasts in
Experiment 2 posted advisories 100% of the time in all three visualization conditions when high
winds were likely. For the students, however, optimal performance was only observed in the box
plot condition. This result, combined with the ease of use evidence reported above, suggests that
the box plot format may be advantageous for those with less expertise. Hence, it might even be
appropriate for presenting forecast uncertainty to the general public. There may have been a
slight advantage for professional forecasters, when using the box plot in low likelihood
situations, in which participants posted 0% advisories in this condition only. This is clearly an
issue that warrants further study.
There are two important differences between the box plot and the other two
visualizations. The first is that in the box plot included specific information about the lower
bound (as well as about the upper bound), which may have counteracted any tendency toward
high bias in wind speed forecasting, especially among the student participants. The second
difference was that, unlike the color-coded charts, the box plot did not provide information about
the weather conditions in the areas surrounding the three forecast sites. Obviously, if these
products were to go operational, box plots for any requested location would be available,

24
however this was not the case in the experiments reported here. Thus, it could be argued that the
people in the two chart conditions had an advantage in this respect, because they had more
information about the overall weather situation. None-the-less, this advantage did not translate
into differences in forecasting performance. The observed increase in advisory postings in the
box plot condition, and the biased wind speed forecast in the upper bound condition in
Experiment 1, could not reasonably be attributed to information about surrounding areas (or lack
thereof).
Among both students and professional forecasters, the margin of error chart, showing the
range of wind speeds associated with each geographic location, was the optimal format for
enhancing the users understanding of the uncertainty involved in the forecast as well as detecting
relative uncertainty. However using this chart may have been more challenging. It is important to
emphasize here that none of the participants, including the professional forecasters, had used any
of these charts previous to their participation in this study. The concept of the margin of error
chart was novel to all participants. Although most people made the best uncertainty estimates
when using the margin of error chart, there were exceptions. Two of the student forecasters and
one professional forecaster had unusually low correlations between rating and range when using
the margin of error chart, suggesting that they had difficulty extracting uncertainty information
from this display. This anomaly, evident only in the margin of error condition, suggests that the
margin of error chart may have been a bit more difficult to comprehend compared to the other
two charts. The low usability rating corroborates this claim. However, for the majority of
participants, once mastered, the margin of error chart was optimal for assessing forecast
uncertainty.

25
Our greatest concern was for the upper bound visualization. This issue is important
because the “worst case scenario” is often used as an example of the kind of uncertainty
information that would be useful to both professional forecasters as well as the general public
(National Research Council, 2006). These results suggest that emphasizing the upper bound or
the worst-case scenario, may introduce unintentional bias into the forecast, especially for novice
forecasters. A high bias makes some sense in this context. Participants may have been
consciously or unconsciously erring on the side caution, in response to the safety issue involved
in the high wind advisory task. Similar biases have been noted in previous work when safety was
an issue (Brown & Murphy, 1987; Keith, 2003). Another possible explanation proposed by some
of the professional forecasters in debriefing was that they were counteracting a known underforecasting bias in the model. However neither of these explanations accounts for the fact that in
Experiment 1 this bias was only significant in the upper bound condition. In Experiment 1 the
weather data in each experimental condition were identical, so the difference could only be due
to visualization format. Thus, the upper bound chart, which emphasized the highest likely wind
speed, appears to have served as a high anchor, drawing wind speed forecasts up. We worry that
this has dangerous implications when translated into a real world setting. An unconscious overforecasting tendency due to visualization format could be compounded by deliberate strategies to
counteract model biases or to enhance safety, and lead, in turn, to an increase in false alarms.
This may ultimately undermine trust in the forecast creating new safety problems due to noncompliance.
In sum, these studies suggest that the optimal format for presenting uncertainty
information may be dependant on both the user and the task. The experiments reported here
demonstrate that forecast uncertainty information can be communicated accurately and

26
efficiently to both professional forecasters as well as to those with less background knowledge. It
is crucial, however, to understand the interaction of the user and the forecasting task with the
visualization format. We are convinced that uncertainty products will be more likely to be used,
and used to good advantage, if these factors are taken into consideration during the development
of such products.

References
Brown, B. G., & Murphy, A. H. (1987). Quantification of uncertainty in fire-weather forecasts:
Some results of operational and experimental forecasting programs. Weather and

27
Forecasting, 2, 190-205.
Chapman, G. B., & Johnson, E. J. (2002). Incorporating the irrelevant: Anchors in judgments of
belief and value. In T. Gilovich, D. Griffin, & D. Kahneman (Eds.), Heuristics and
biases: The psychology of intuitive judgment (pp. 120–138). Cambridge: Cambridge
University Press.
Camerer, C., & Ho, T. (1994). Violations of the betweenness axiom and nonlinearity in
probability. Journal of Risk and Uncertainty, 8, 167-196.
deElia, R., & Laprise, R. (2005). Diversity in interpretations of probability: Implications for
weather forecasting. American Meteorological Society, 133(5), 1129-1143.
Endsley, M., Bolté, B., & Jones, D. (2003). Designing for situation awareness: An approach to
user-centered design. Taylor, & Francis, London.
Epstein, E. S. (1969). Stochastic dynamic prediction. Tellus, 21, 739-759.
Gneiting, T., & Raftery, A. E. (2005). Weather forecasting with ensemble methods. Science, 310,
248-249.
Gonzalez, R., & Wu, G. (1999). On the form of the probability weighting function. Cognitive
Psychology, 38, 129-166.
Joslyn, S., & Jones, D. (in press). Strategies in naturalistic decision-making: A cognitive task
analysis of naval weather forecasting. In J. M. Schraagen (Eds.), Naturalistic decision
making and macrocognition. Ashgate Publishing. Available at
http://www.stat.washington. edu/ MURI/.
Heath, C., & Luff, P. (2000). Technology in action. Cambridge: Cambridge University Press.
Kahneman, D., & Tversky, A. (1979). Prospect theory: An analysis of decision under risk.
Econometrica, 47, 263-291.

28
Koehler, D. J., Brenner, L., & Griffin, D. (2002). The calibration of expert judgment: Heuristics
and biases beyond the laboratory. In T. Gilovich, D. Griffin, & D. Kahneman (Eds.),
Heuristics and biases: The psychology of intuitive judgment. New York: Cambridge
University Press.
Keith, R. (2003). Optimization of value of aerodrome forecasts. Weather and Forecasting, 18(5),
808-824.
Leith C. E. (1974). Theoretical skill of Monte Carlo forecasts. Monthly Weather Review, 102,
409-418.
Murphy, A. H., & Winkler, R. L. (1974a). Credible interval temperature forecasting: some
experimental results. Monthly Weather Review, 162(11), 784-794.
Murphy, A. H., & Winkler, R. L. (1974b). Subjective probability forecasting experiments in
meteorology: some preliminary results. Bulletin of the American Meteorological Society,
55(10), 1206-1216.
Murphy, A. H., & Winkler, R.L. (1977). Can weather forecasters formulate reliable forecasts of
precipitation and temperature? National weather digest, 2, 2-9.
National Research Council. (2006). Completing the forecast: Characterizing and
communicating uncertainty for better decisions using weather and climate forecasts.
Committee on Estimating and Communicating Uncertainty in Weather and Climate
Forecasts. Washington, DC: The National Academies Press.
Norman, D. A. (1986). Cognitive engineering. In D. A. Norman, & S. W. Draper (Eds.), User
centered system design: New Perspectives on human-computer interaction (pp. 31-61).
Lawrence Erlbaum As-sociates, Hillsdale, N.J.
Pielke, R.A., Jr. (1999). Nine fallacies of floods. Climatic Change, 42(2), 413-438.

29
Pliske, R., Klinger, D., Hutton, R., Crandall, B., Knight, B., & Klein, G. (1997). Understanding
skilled weather forecasting: Implications for training and design of forecasting tools.
Technical Report AL/HR-CR-1997-003, Air Force Material Command, Armstrong
Laboratoy, Human Systems Directorate.
Raftery, A. E., Gneiting, T., Balabdaoui, F., & Polakowski, M. (2005). Using Bayesian model
averaging to calibrate forecast ensembles. Monthly Weather Review, 133, 1155-1174.
Sloughter, J. M., Raftery, A. E., Gneiting, T., & Fraley, F. (in press). Probabilistic quantitative
precipitation forecasting using Bayesian model averaging. Monthly Weather Review.
Available at http://www.stat.washington.edu/tilmann/publications.html.
Tversky, A., & Kahneman, D. (1974). Judgment under uncertainty: Heuristics and biases.
Science, 185, 1124-1131.
Tversky, A., & Kahneman, D. (1992). Advances in prospect theory: Cumulative representations
of uncertainty. Journal of Risk and Uncertainty, 5, 297–323.

Footnotes
1

The range was not calculated from the lower bound because, as we explained to

participants, it is almost always between zero and two knots and is therefore not very
informative.

30
2

No significant group differences were detected using this variable so it will not be

discussed further.
3

Similar results were obtained when absolute error rather than mean error was used as

the dependant variable (F (2, 23) = 17.79, p < .001). As before, Tukey’s HSD revealed that the
absolute reading errors made in the box plot condition were significantly smaller than those
made in both the margin of error and upper bound conditions, at p < .01.
4

Because these were hypothetical forecasts no actual verification was possible.

5

One subject was omitted from this analysis because he only completed the box plot part

of the test due to color blindness that prevented him from distinguishing between the other two
experimental charts.

Table 1
Percentage of Advisories Posted in High, Medium and Low Likelihood of Wind Speeds –
Experiment 1

31
Margin of Error

Upper Bound

Box Plot

High winds

98.4%

94.4%

91.7%

Medium winds

62.5%

55.6%

52.4%

3.8%

4.0%

2.4%

Low winds

N = 26.

Table 2
Means and Standard Deviations of Participants’ Estimated Forecast Accuracy – Experiment 1

Margin of error

Upper bound

Box plot

32
Mean

SD

Mean

Median understandability

9.43

.79

8.38

Median ease of use

6.0

2.0

Experimental understandability

9.29

Experimental ease of use

SD

Mean

SD

2.20

8.38

1.58

7.38

2.07

7.38

2.13

.76

8.88

1.36

8.50

.76

5.57

1.62

7.38

2.07

8.38

1.19

% Accuracy within 2 knots

38.5

13.5

55.0

20.7

52.5

26.0

% Accuracy within 5 knots

57.1

19.8

78.7

14.6

77.5

17.5

% Accuracy within 10 knots

85.7

12.7

96.2

7.4

95.0

5.3

% Accurate postings

57.1

13.8

72.5

11.6

73.7

7.4

Table 3
Percentage of Advisories Posted in High, Medium and Low Likelihood of Wind Speeds –
Experiment 1

33
Margin of Error

Upper Bound

Box Plot

High winds

100%

100%

100%

Medium winds

54.2%

54.2%

50%

3.1%

6.2%

0%

Low winds

N = 8.

Table 4
Means and Standard Deviations of Participants’ Estimated Forecast Accuracy – Experiment 2

Margin of error

Upper bound

Box plot

34
Mean

SD

Mean

SD

Mean

SD

Median understandability

8.29

1.80

8.57

1.72

9.13

.83

Median ease of use

6.14

1.86

6.29

1.80

6.50

2.88

Experimental understandability

8.43

.98

8.43

1.62

9.25

1.39

Experimental ease of use

6.57

2.22

6.29

1.80

8.88

1.46

% Accuracy within 2 knots

23.8

16.2

28.6

23.0

37.5

21.4

% Accuracy within 5 knots

47.6

17.8

52.4

17.8

66.7

25.2

% Accuracy within 10 knots

85.7

26.2

85.7

26.2

91.6

23.6

% Accurate postings

71.5

12.6

66.7

.00

75.0

15.4

Note. Participants were asked to estimate how many of the three forecasts they made for each
location were within 2, 5 and 10 knots.

Figure Captions
Figure 1. Percent of advisories posted by participants in Experiment 1, when medium level wind
speeds were expected.

35

Figures
Figure 1

36

Percent of Advisories Posted

100
90
80

Margin of Error
Upper Bound

70

Box Plot

60
50
40
30
20
10
0
15.2

16.46

17.29

18.2

18.7

Computer-Model Median Wind Speeds

Appendix A
The Median chart

18.9

18.96

37

Appendix B
The upper bound chart

38

Instructions for reading the upper bound were: “The colors on this chart indicate wind speeds at
the upper bound for each location. You read the colors using the legend just as you did for the
median wind forecast chart. The difference is that these are the highest predicted wind speeds at
each location instead of the median wind speed.”
Instructions for reading the range were: “The upper bound and the median forecast are used to
get an estimate of the uncertainty in our forecast. We will ignore the lower bound, as it is almost
always somewhere between zero and two knots, and is therefore not very informative. The first
step is to calculate the range. By subtracting the median forecast from the upper bound, we get a
range from the median to the upper bound.”

Appendix C
The Margin of Error chart

39

Instructions for reading the range were: “This chart doesn’t show the actual forecasted wind
speed for any location, rather, it tells you how much uncertainty there is in the forecast for each
location. The colors on this chart indicate the range of wind speed in knots at each location. The
colors do NOT indicate the predicted wind speeds as with the median forecast. It uses different
colors than the median wind speed chart so that you don’t get confused. The color bar legend to
the right of the display shows how to interpret the colors in knots.”
Instructions for obtaining the upper bound were: “You can also figure out the upper bound of the
range of forecasts, by using the margin of error chart and the median wind speed chart together.
Because the margin of error is the difference between the median wind speed and the upper
bound, you add the margin of error to the median forecast to get the upper bound.”
Appendix D
The Box Plot chart

40
Location A

Lower Bound

Median wind speed

Upper Bound

Instructions for reading the upper bound were: “The numbers at the bottom indicate wind speed
in knots. The line in the middle of the box indicates the median forecast that was shown on the
chart you just learned to read. The box encompasses the span of forecasts within which the
actual wind speeds will be included 1 time out of every 2. The vertical lines on the very ends
(right and left) indicate the upper and the lower bound.”
Instructions for how to obtain the range were: “The upper bound and the median forecast are
used to get an estimate of the uncertainty in our forecast. The first step is to calculate the range.
By subtracting the median forecast (the line in the middle of the box) from the upper bound (the
line on the right) we get a range from the median to the upper bound.”

Appendix E
Questions used in the test phase

41

Forecast number 1
Uncertainty level

1. Please rate the overall uncertainty for the computer model forecast for this date.
Location A
2. According to the computer model what is the median wind speed forecast at Location A?

Uncertainty level

3. Please rate the uncertainty in the computer model forecast for Location A
4. What is your forecast for the wind speed at Location A?
5. If your forecast is off, will it likely result in:

Wind speed level

6. Fill in the blank: Actual speeds at Location A will exceed _______ knots one time in 10.
7. Do you wish to issue a small craft advisory for location A (for wind speeds equal to or greater than 20 knots)?

Choose

8. Fill in the blank: There is/are ___ chance(s) in 10 that my small craft advisory decision will turn out to be right.
Submit

Appendix F

42
Underlying median wind speeds, ranges and upper bound information presented to participants
in Experiment 2

Upper bound condition
Locations
Median
Upper Bound
Range

a
25.6
40.0
14.4

Locations
Medium
Median
winds
Upper Bound
Range

a
19.7
40.0
20.3

Locations
Median
Upper Bound
Range

a
10.5
25.8
15.3

High
winds

Low
winds

12/13/05
b
c
26.3 15.2
40.0 34.6
13.7 19.4
01/10/05
b
c
16.2
7.4
36.4 22.2
20.2 14.8
02/16/05
b
c
9.7
7.2
24.4 20.2
14.7 13.0

Margin of error condition

mean
22.4
38.2
15.8

a
23.2
39.2
16.0

mean
14.4
32.9
18.4

a
18.7
39.9
21.2

mean
9.1
23.5
14.3

a
12.0
27.0
15.0

3/8/2005
b
c
20.6 16.5
38.5 32.1
17.9 15.6
01/09/05
b
c
18.9
7.1
40.0 21.6
21.1 14.5
03/24/05
b
c
10.9
5.8
24.3 18.8
13.4 13.0

Box plot condition

mean
20.1
36.6
16.5

a
29.7
40.0
10.3

mean
14.9
33.8
18.9

a
18.3
38.0
19.7

mean
9.6
23.4
13.8

a
12.7
29.0
16.3

12/18/05
b
c
24.0
9.8
40.0 26.0
16.0 16.2
12/03/05
b
c
15.2
4.7
33.7 16.2
18.5 11.5
02/13/05
b
c
12.4 11.5
28.6 27.0
16.2 15.5

mean
21.2
35.3
14.2
mean
12.7
29.3
16.6
mean
12.2
28.2
16.0

